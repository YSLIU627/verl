Loading the DigitalLearningGmbH/MATH-lighteval dataset from huggingface...
Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 250.89ba/s]
Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 385.91ba/s]
2025-02-23 01:33:26,694	INFO worker.py:1841 -- Started a local Ray instance.
[36m(main_task pid=3000471)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=3000471)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=3000471)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3000471)[0m                                                  'grad_offload': False,
[36m(main_task pid=3000471)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=3000471)[0m                                                  'param_offload': False,
[36m(main_task pid=3000471)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3000471)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=3000471)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=3000471)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=3000471)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=3000471)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3000471)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=3000471)[0m                                            'total_training_steps': -1,
[36m(main_task pid=3000471)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=3000471)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=3000471)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=3000471)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=3000471)[0m                                  'ppo_micro_batch_size_per_gpu': 32,
[36m(main_task pid=3000471)[0m                                  'ppo_mini_batch_size': 256,
[36m(main_task pid=3000471)[0m                                  'shuffle': False,
[36m(main_task pid=3000471)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=3000471)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3000471)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=3000471)[0m                                  'use_kl_loss': True},
[36m(main_task pid=3000471)[0m                        'hybrid_engine': True,
[36m(main_task pid=3000471)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3000471)[0m                                  'external_lib': None,
[36m(main_task pid=3000471)[0m                                  'override_config': {},
[36m(main_task pid=3000471)[0m                                  'path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3000471)[0m                                  'use_remove_padding': True},
[36m(main_task pid=3000471)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=3000471)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3000471)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3000471)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=3000471)[0m                                'log_prob_micro_batch_size_per_gpu': 80,
[36m(main_task pid=3000471)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3000471)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=3000471)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=3000471)[0m                                    'do_sample': True,
[36m(main_task pid=3000471)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=3000471)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=3000471)[0m                                    'enforce_eager': True,
[36m(main_task pid=3000471)[0m                                    'free_cache_engine': True,
[36m(main_task pid=3000471)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=3000471)[0m                                    'ignore_eos': False,
[36m(main_task pid=3000471)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=3000471)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3000471)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=3000471)[0m                                    'log_prob_micro_batch_size_per_gpu': 80,
[36m(main_task pid=3000471)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3000471)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=3000471)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=3000471)[0m                                    'n': 2,
[36m(main_task pid=3000471)[0m                                    'name': 'vllm',
[36m(main_task pid=3000471)[0m                                    'prompt_length': 512,
[36m(main_task pid=3000471)[0m                                    'response_length': 256,
[36m(main_task pid=3000471)[0m                                    'temperature': 1.0,
[36m(main_task pid=3000471)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=3000471)[0m                                    'top_k': -1,
[36m(main_task pid=3000471)[0m                                    'top_p': 1}},
[36m(main_task pid=3000471)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=3000471)[0m                'gamma': 1.0,
[36m(main_task pid=3000471)[0m                'kl_ctrl': {'kl_coef': 0.001,
[36m(main_task pid=3000471)[0m                            'kl_coef_correction': 0.01,
[36m(main_task pid=3000471)[0m                            'type': 'fixed'},
[36m(main_task pid=3000471)[0m                'kl_penalty': 'kl',
[36m(main_task pid=3000471)[0m                'lam': 1.0,
[36m(main_task pid=3000471)[0m                'optimism_coef': 0.1,
[36m(main_task pid=3000471)[0m                'optimistic_actor': True,
[36m(main_task pid=3000471)[0m                'optimistic_critic': False},
[36m(main_task pid=3000471)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=3000471)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3000471)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=3000471)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=3000471)[0m             'grad_clip': 1.0,
[36m(main_task pid=3000471)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3000471)[0m                       'external_lib': None,
[36m(main_task pid=3000471)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3000471)[0m                                       'grad_offload': False,
[36m(main_task pid=3000471)[0m                                       'optimizer_offload': False,
[36m(main_task pid=3000471)[0m                                       'param_offload': False,
[36m(main_task pid=3000471)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3000471)[0m                       'override_config': {},
[36m(main_task pid=3000471)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=3000471)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3000471)[0m                       'use_remove_padding': False},
[36m(main_task pid=3000471)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=3000471)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3000471)[0m                       'min_lr_ratio': None,[36m(main_task pid=3000471)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=3000471)[0m No module named 'vllm._version'
[36m(main_task pid=3000471)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3000955)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3000955)[0m No module named 'vllm._version'
[36m(pid=3000955)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3001146)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3001146)[0m No module named 'vllm._version'
[36m(pid=3001146)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=3001146)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3001146)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3001146)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.81it/s]
[36m(WorkerDict pid=3001146)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.09it/s]
[36m(WorkerDict pid=3000955)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(pid=3001149)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=3001149)[0m No module named 'vllm._version'[32m [repeated 6x across cluster][0m
[36m(pid=3001149)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3001148)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3000955)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3000955)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.79it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3000955)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.11it/s][32m [repeated 7x across cluster][0m

[36m(main_task pid=3000471)[0m                       'total_training_steps': -1,
[36m(main_task pid=3000471)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=3000471)[0m             'ppo_epochs': 1,
[36m(main_task pid=3000471)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=3000471)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=3000471)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=3000471)[0m             'ppo_mini_batch_size': 256,
[36m(main_task pid=3000471)[0m             'shuffle': False,
[36m(main_task pid=3000471)[0m             'strategy': 'fsdp',
[36m(main_task pid=3000471)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3000471)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=3000471)[0m  'data': {'custom_temp_dir': '/wang_ssd/zhihanliu/tmp/ray/',
[36m(main_task pid=3000471)[0m           'max_prompt_length': 512,
[36m(main_task pid=3000471)[0m           'max_response_length': 256,
[36m(main_task pid=3000471)[0m           'prompt_key': 'prompt',
[36m(main_task pid=3000471)[0m           'return_raw_chat': False,
[36m(main_task pid=3000471)[0m           'return_raw_input_ids': False,
[36m(main_task pid=3000471)[0m           'save_ppo_rollouts_path': '',
[36m(main_task pid=3000471)[0m           'shuffle': True,
[36m(main_task pid=3000471)[0m           'tokenizer': None,
[36m(main_task pid=3000471)[0m           'train_batch_size': 256,
[36m(main_task pid=3000471)[0m           'train_files': '/wang_ssd/zhihanliu/data//train.parquet',
[36m(main_task pid=3000471)[0m           'val_batch_size': 1024,
[36m(main_task pid=3000471)[0m           'val_files': '/wang_ssd/zhihanliu/data//test.parquet'},
[36m(main_task pid=3000471)[0m  'reward_model': {'enable': False,
[36m(main_task pid=3000471)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3000471)[0m                   'max_length': None,
[36m(main_task pid=3000471)[0m                   'micro_batch_size': None,
[36m(main_task pid=3000471)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=3000471)[0m                   'model': {'external_lib': None,
[36m(main_task pid=3000471)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3000471)[0m                                             'min_num_params': 0,
[36m(main_task pid=3000471)[0m                                             'param_offload': False},
[36m(main_task pid=3000471)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3000471)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=3000471)[0m                             'use_remove_padding': False},
[36m(main_task pid=3000471)[0m                   'reward_manager': 'naive',
[36m(main_task pid=3000471)[0m                   'strategy': 'fsdp',
[36m(main_task pid=3000471)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3000471)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=3000471)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=3000471)[0m              'default_hdfs_dir': None,
[36m(main_task pid=3000471)[0m              'default_local_dir': 'checkpoints/Qwen2.5-3B-Instruct-GRPO/debug',
[36m(main_task pid=3000471)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=3000471)[0m              'experiment_name': 'debug',
[36m(main_task pid=3000471)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=3000471)[0m              'n_gpus_per_node': 8,
[36m(main_task pid=3000471)[0m              'nnodes': 1,
[36m(main_task pid=3000471)[0m              'project_name': 'Qwen2.5-3B-Instruct-GRPO',
[36m(main_task pid=3000471)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=3000471)[0m              'resume_from_path': False,
[36m(main_task pid=3000471)[0m              'resume_mode': 'auto',
[36m(main_task pid=3000471)[0m              'save_freq': 10,
[36m(main_task pid=3000471)[0m              'test_freq': 10,
[36m(main_task pid=3000471)[0m              'total_epochs': 2,
[36m(main_task pid=3000471)[0m              'total_training_steps': None,
[36m(main_task pid=3000471)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=3000471)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=3000471)[0m original dataset len: 7500
[36m(main_task pid=3000471)[0m filter dataset len: 7425
[36m(main_task pid=3000471)[0m original dataset len: 5000
[36m(main_task pid=3000471)[0m filter dataset len: 4972
[36m(main_task pid=3000471)[0m Size of train dataloader: 29
[36m(main_task pid=3000471)[0m Size of val dataloader: 1
[36m(main_task pid=3000471)[0m Total training steps: 58
[36m(WorkerDict pid=3000955)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3000955)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3000955)[0m   "architectures": [
[36m(WorkerDict pid=3000955)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3000955)[0m   ],
[36m(WorkerDict pid=3000955)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3000955)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3000955)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3000955)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3000955)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3000955)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3000955)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3000955)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3000955)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3000955)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3000955)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3000955)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3000955)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3000955)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3000955)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3000955)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3000955)[0m   "sliding_window": null,
[36m(WorkerDict pid=3000955)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3000955)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3000955)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3000955)[0m   "use_cache": true,
[36m(WorkerDict pid=3000955)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3000955)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3000955)[0m }
[36m(WorkerDict pid=3000955)[0m 
[36m(WorkerDict pid=3000955)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3000955)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3000955)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fc5d0a27ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fc5d0a27d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3000955)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3000955)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3000955)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3000955)[0m   "architectures": [
[36m(WorkerDict pid=3000955)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3000955)[0m   ],
[36m(WorkerDict pid=3000955)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3000955)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3000955)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3000955)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3000955)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3000955)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3000955)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3000955)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3000955)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3000955)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3000955)[0m   "num_hidden_layers": 36,[36m(WorkerDict pid=3001146)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001150)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3000955)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.27s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3001148)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
[36m(WorkerDict pid=3001152)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
[36m(WorkerDict pid=3001148)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3001148)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3000955)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.28s/it][32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3001150)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=3000955)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3000955)[0m   warnings.warn(
[36m(WorkerDict pid=3001152)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3001152)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001152)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 7x across cluster][0m
[36m(main_task pid=3000471)[0m wandb: Currently logged in as: zhliu0627 (northwestern_university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=3000471)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=3000471)[0m wandb: Tracking run with wandb version 0.19.5
[36m(main_task pid=3000471)[0m wandb: Run data is saved locally in /wang_ssd/zhihanliu/zhihan/verl/wandb/run-20250223_013433-57qe07wh
[36m(main_task pid=3000471)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=3000471)[0m wandb: Syncing run debug
[36m(main_task pid=3000471)[0m wandb: ⭐️ View project at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO
[36m(main_task pid=3000471)[0m wandb: 🚀 View run at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO/runs/57qe07wh

[36m(WorkerDict pid=3000955)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3000955)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3000955)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3000955)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3000955)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3000955)[0m   "sliding_window": null,
[36m(WorkerDict pid=3000955)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3000955)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3000955)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3000955)[0m   "use_cache": true,
[36m(WorkerDict pid=3000955)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3000955)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3000955)[0m }
[36m(WorkerDict pid=3000955)[0m 
[36m(WorkerDict pid=3001150)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f0a612efec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f0a612efd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3000955)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3001150)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3000955)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fc5d0a27ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fc5d0a27d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3001148)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f3be8c0fec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f3be8c0fd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3001148)[0m Total steps: 58, num_warmup_steps: 0
[36m(WorkerDict pid=3001148)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3001150)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f0a612efec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f0a612efd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3001146)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3000955)[0m Before building vllm rollout, memory allocated (GB): 1.4370465278625488, memory reserved (GB): 5.68359375
[36m(WorkerDict pid=3001148)[0m INFO 02-23 01:34:22 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3001148)[0m INFO 02-23 01:34:22 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=3001148)[0m WARNING 02-23 01:34:22 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3001148)[0m local rank 0
[36m(WorkerDict pid=3001148)[0m INFO 02-23 01:34:22 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3001152)[0m Total steps: 58, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001152)[0m Actor use_remove_padding=True[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3000955)[0m INFO 02-23 01:34:24 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3000955)[0m INFO 02-23 01:34:24 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3001147)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3001147)[0m INFO 02-23 01:34:24 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f5c10c10dd0>, local_subscribe_port=46525, remote_subscribe_port=None)
[36m(WorkerDict pid=3000955)[0m before init cache memory allocated: 4.700148736GB, reserved: 4.79199232GB
[36m(WorkerDict pid=3000955)[0m after init cache memory allocated: 30.89777152GB, reserved: 30.989615104GB
[36m(WorkerDict pid=3001152)[0m INFO 02-23 01:34:23 config.py:887] Defaulting to use ray for distributed inference[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001152)[0m INFO 02-23 01:34:23 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001152)[0m WARNING 02-23 01:34:23 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001152)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001149)[0m INFO 02-23 01:34:25 selector.py:115] Using XFormers backend.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3001150)[0m INFO 02-23 01:34:24 utils.py:1008] Found nccl from library libnccl.so.2[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001150)[0m INFO 02-23 01:34:24 pynccl.py:63] vLLM is using nccl==2.20.5[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001149)[0m NCCL version 2.20.5+cuda12.4[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3000955)[0m kwargs: {'n': 2, 'logprobs': 1, 'max_tokens': 256, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3000955)[0m After building vllm rollout, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(WorkerDict pid=3000955)[0m After building sharding manager, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(WorkerDict pid=3001149)[0m INFO 02-23 01:34:25 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f99f0157dd0>, local_subscribe_port=44359, remote_subscribe_port=None)[32m [repeated 3x across cluster][0m
[36m(main_task pid=3000471)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(main_task pid=3000471)[0m Checkpoint tracker file does not exist: %s /wang_ssd/zhihanliu/zhihan/verl/checkpoints/Qwen2.5-3B-Instruct-GRPO/debug/latest_checkpointed_iteration.txt
[36m(main_task pid=3000471)[0m Training from scratch
[36m(main_task pid=3000471)[0m validation generation end
[36m(WorkerDict pid=3001149)[0m kwargs: {'n': 2, 'logprobs': 1, 'max_tokens': 256, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
[36m(main_task pid=3000471)[0m <|im_start|>system
[36m(main_task pid=3000471)[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
[36m(main_task pid=3000471)[0m <|im_start|>user
[36m(main_task pid=3000471)[0m In triangle $ABC$,
[36m(main_task pid=3000471)[0m \[2a^2 + 4b^2 + c^2 = 4ab + 2ac.\]Compute the numerical value of $\cos B.$ Let's think step by step and output the final answer within \boxed{}.<|im_end|>
[36m(main_task pid=3000471)[0m <|im_start|>assistant
[36m(main_task pid=3000471)[0m To find the value of \(\cos B\) in triangle \(ABC\) given the equation \(2a^2 + 4b^2 + c^2 = 4ab + 2ac\), we will use the Law of Cosines and some algebraic manipulation.
[36m(main_task pid=3000471)[0m 
[36m(main_task pid=3000471)[0m First, recall the Law of Cosines:
[36m(main_task pid=3000471)[0m \[
[36m(main_task pid=3000471)[0m c^2 = a^2 + b^2 - 2ab \cos C
[36m(main_task pid=3000471)[0m \]
[36m(main_task pid=3000471)[0m \[
[36m(main_task pid=3000471)[0m b^2 = a^2 + c^2 - 2ac \cos B
[36m(main_task pid=3000471)[0m \]
[36m(main_task pid=3000471)[0m \[
[36m(main_task pid=3000471)[0m a^2 = b^2 + c^2 - 2bc \cos A
[36m(main_task pid=3000471)[0m \]
[36m(main_task pid=3000471)[0m 
[36m(main_task pid=3000471)[0m We start with the given equation:
[36m(main_task pid=3000471)[0m \[
[36m(main_task pid=3000471)[0m 2a^2 + 4b^2 + c^2 = 4ab + 2ac
[36m(main_task pid=3000471)[0m \][36m(main_task pid=3000471)[0m wandb:                                                                                
[36m(WorkerDict pid=3001149)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3001149)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(main_task pid=3000471)[0m wandb: 
[36m(main_task pid=3000471)[0m wandb: Run history:
[36m(main_task pid=3000471)[0m wandb: val/test_score/DigitalLearningGmbH/MATH-lighteval ▁
[36m(main_task pid=3000471)[0m wandb: 
[36m(main_task pid=3000471)[0m wandb: Run summary:
[36m(main_task pid=3000471)[0m wandb: val/test_score/DigitalLearningGmbH/MATH-lighteval 0.09191
[36m(main_task pid=3000471)[0m wandb: 
[36m(main_task pid=3000471)[0m wandb: 🚀 View run debug at: https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO/runs/57qe07wh
[36m(main_task pid=3000471)[0m wandb: ⭐️ View project at: https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO
[36m(main_task pid=3000471)[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[36m(main_task pid=3000471)[0m wandb: Find logs at: ./wandb/run-20250223_013433-57qe07wh/logs

[36m(main_task pid=3000471)[0m 
[36m(main_task pid=3000471)[0m Rearrange the equation to bring all terms to one side:
[36m(main_task pid=3000471)[0m \[
[36m(main_task pid=3000471)[0m 2a^2 + 4b^2 + c^2 - 4ab - 2ac = 0
[36m(main_task pid=3000471)[0m \]
[36m(main_task pid=3000471)[0m 
[36m(main_task pid=3000471)[0m To simplify, we will try to express this in a form that resembles the Law of Cosines. Let's consider the expression \(a^2 + b^2 + c^2 - ab - ac - bc\
[36m(main_task pid=3000471)[0m ('Initial validation metrics: '
[36m(main_task pid=3000471)[0m  "{'val/test_score/DigitalLearningGmbH/MATH-lighteval': 0.0919147224456959}")
[36m(main_task pid=3000471)[0m step:0 - val/test_score/DigitalLearningGmbH/MATH-lighteval:0.092
[36m(main_task pid=3000471)[0m '******************************'
[36m(main_task pid=3000471)[0m ('torch.Size([512, 256]), torch.Size([256, 2, 256]), torch.Size([256, 1, '
[36m(main_task pid=3000471)[0m  '256]), torch.Size([256, 2, 256]), torch.Size([256, 256])')
[36m(main_task pid=3000471)[0m '******************************'
[36m(main_task pid=3000471)[0m TensorDict(
[36m(main_task pid=3000471)[0m     fields={
[36m(main_task pid=3000471)[0m         optimistic_advantages: Tensor(shape=torch.Size([512, 256]), device=cpu, dtype=torch.float32, is_shared=False)},
[36m(main_task pid=3000471)[0m     batch_size=torch.Size([512]),
[36m(main_task pid=3000471)[0m     device=None,
[36m(main_task pid=3000471)[0m     is_shared=False)
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'algorithm.optimism_coef=0.1', 'algorithm.optimistic_actor=True', 'data.train_files=/wang_ssd/zhihanliu/data//train.parquet', 'data.val_files=/wang_ssd/zhihanliu/data//test.parquet', 'data.custom_temp_dir=/wang_ssd/zhihanliu/tmp/ray/', 'data.train_batch_size=256', 'data.val_batch_size=1024', 'data.max_prompt_length=512', 'data.max_response_length=256', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=256', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=32', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=80', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=2', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=80', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=Qwen2.5-3B-Instruct-GRPO', 'trainer.experiment_name=debug', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=10', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 240, in <module>
    main()
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 128, in main
    run_ppo(config)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 142, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): [36mray::main_task()[39m (pid=3000471, ip=10.14.38.55)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 236, in main_task
    trainer.fit()
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/ppo/ray_trainer.py", line 979, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch, optimism = self.optimistic_actor )
                                                                        ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RayPPOTrainer' object has no attribute 'optimistic_actor'
Loading the DigitalLearningGmbH/MATH-lighteval dataset from huggingface...
Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 266.97ba/s]
Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 398.83ba/s]
2025-02-23 11:43:12,678	INFO worker.py:1841 -- Started a local Ray instance.
[36m(main_task pid=3221873)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=3221873)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=3221873)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3221873)[0m                                                  'grad_offload': False,
[36m(main_task pid=3221873)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=3221873)[0m                                                  'param_offload': False,
[36m(main_task pid=3221873)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3221873)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=3221873)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=3221873)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=3221873)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=3221873)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3221873)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=3221873)[0m                                            'total_training_steps': -1,
[36m(main_task pid=3221873)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=3221873)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=3221873)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=3221873)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=3221873)[0m                                  'ppo_micro_batch_size_per_gpu': 16,
[36m(main_task pid=3221873)[0m                                  'ppo_mini_batch_size': 256,
[36m(main_task pid=3221873)[0m                                  'shuffle': False,
[36m(main_task pid=3221873)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=3221873)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3221873)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=3221873)[0m                                  'use_kl_loss': True},
[36m(main_task pid=3221873)[0m                        'hybrid_engine': True,
[36m(main_task pid=3221873)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3221873)[0m                                  'external_lib': None,
[36m(main_task pid=3221873)[0m                                  'override_config': {},
[36m(main_task pid=3221873)[0m                                  'path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3221873)[0m                                  'use_remove_padding': True},
[36m(main_task pid=3221873)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=3221873)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3221873)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3221873)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=3221873)[0m                                'log_prob_micro_batch_size_per_gpu': 64,
[36m(main_task pid=3221873)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3221873)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=3221873)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=3221873)[0m                                    'do_sample': True,
[36m(main_task pid=3221873)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=3221873)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=3221873)[0m                                    'enforce_eager': True,
[36m(main_task pid=3221873)[0m                                    'free_cache_engine': True,
[36m(main_task pid=3221873)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=3221873)[0m                                    'ignore_eos': False,
[36m(main_task pid=3221873)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=3221873)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3221873)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=3221873)[0m                                    'log_prob_micro_batch_size_per_gpu': 64,
[36m(main_task pid=3221873)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3221873)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=3221873)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=3221873)[0m                                    'n': 5,
[36m(main_task pid=3221873)[0m                                    'name': 'vllm',
[36m(main_task pid=3221873)[0m                                    'prompt_length': 1024,
[36m(main_task pid=3221873)[0m                                    'response_length': 2048,
[36m(main_task pid=3221873)[0m                                    'temperature': 1.0,
[36m(main_task pid=3221873)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=3221873)[0m                                    'top_k': -1,
[36m(main_task pid=3221873)[0m                                    'top_p': 1}},
[36m(main_task pid=3221873)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=3221873)[0m                'gamma': 1.0,
[36m(main_task pid=3221873)[0m                'kl_ctrl': {'kl_coef': 0.001,
[36m(main_task pid=3221873)[0m                            'kl_coef_correction': 0.01,
[36m(main_task pid=3221873)[0m                            'type': 'fixed'},
[36m(main_task pid=3221873)[0m                'kl_penalty': 'kl',
[36m(main_task pid=3221873)[0m                'lam': 1.0,
[36m(main_task pid=3221873)[0m                'optimism_coef': 0.0,
[36m(main_task pid=3221873)[0m                'optimistic_actor': False,
[36m(main_task pid=3221873)[0m                'optimistic_critic': False},
[36m(main_task pid=3221873)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=3221873)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3221873)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=3221873)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=3221873)[0m             'grad_clip': 1.0,
[36m(main_task pid=3221873)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3221873)[0m                       'external_lib': None,
[36m(main_task pid=3221873)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3221873)[0m                                       'grad_offload': False,
[36m(main_task pid=3221873)[0m                                       'optimizer_offload': False,
[36m(main_task pid=3221873)[0m                                       'param_offload': False,
[36m(main_task pid=3221873)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3221873)[0m                       'override_config': {},
[36m(main_task pid=3221873)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=3221873)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3221873)[0m                       'use_remove_padding': False},
[36m(main_task pid=3221873)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=3221873)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3221873)[0m                       'min_lr_ratio': None,[36m(main_task pid=3221873)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=3221873)[0m No module named 'vllm._version'
[36m(main_task pid=3221873)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3222512)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3222512)[0m No module named 'vllm._version'
[36m(pid=3222512)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3222775)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3222775)[0m No module named 'vllm._version'
[36m(pid=3222775)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=3222775)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3222775)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3222775)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.82it/s]
[36m(WorkerDict pid=3222775)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.06it/s]

[36m(main_task pid=3221873)[0m                       'total_training_steps': -1,
[36m(main_task pid=3221873)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=3221873)[0m             'ppo_epochs': 1,
[36m(main_task pid=3221873)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=3221873)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=3221873)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=3221873)[0m             'ppo_mini_batch_size': 256,
[36m(main_task pid=3221873)[0m             'shuffle': False,
[36m(main_task pid=3221873)[0m             'strategy': 'fsdp',
[36m(main_task pid=3221873)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3221873)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=3221873)[0m  'data': {'custom_temp_dir': '/wang_ssd/zhihanliu/tmp/ray/',
[36m(main_task pid=3221873)[0m           'max_prompt_length': 1024,
[36m(main_task pid=3221873)[0m           'max_response_length': 2048,
[36m(main_task pid=3221873)[0m           'prompt_key': 'prompt',
[36m(main_task pid=3221873)[0m           'return_raw_chat': False,
[36m(main_task pid=3221873)[0m           'return_raw_input_ids': False,
[36m(main_task pid=3221873)[0m           'save_ppo_rollouts_path': '',
[36m(main_task pid=3221873)[0m           'shuffle': True,
[36m(main_task pid=3221873)[0m           'tokenizer': None,
[36m(main_task pid=3221873)[0m           'train_batch_size': 1024,
[36m(main_task pid=3221873)[0m           'train_files': '/wang_ssd/zhihanliu/data//train.parquet',
[36m(main_task pid=3221873)[0m           'val_batch_size': 1024,
[36m(main_task pid=3221873)[0m           'val_files': '/wang_ssd/zhihanliu/data//test.parquet'},
[36m(main_task pid=3221873)[0m  'reward_model': {'enable': False,
[36m(main_task pid=3221873)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3221873)[0m                   'max_length': None,
[36m(main_task pid=3221873)[0m                   'micro_batch_size': None,
[36m(main_task pid=3221873)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=3221873)[0m                   'model': {'external_lib': None,
[36m(main_task pid=3221873)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3221873)[0m                                             'min_num_params': 0,
[36m(main_task pid=3221873)[0m                                             'param_offload': False},
[36m(main_task pid=3221873)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3221873)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=3221873)[0m                             'use_remove_padding': False},
[36m(main_task pid=3221873)[0m                   'reward_manager': 'naive',
[36m(main_task pid=3221873)[0m                   'strategy': 'fsdp',
[36m(main_task pid=3221873)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3221873)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=3221873)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=3221873)[0m              'default_hdfs_dir': None,
[36m(main_task pid=3221873)[0m              'default_local_dir': 'checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH_baseline',
[36m(main_task pid=3221873)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=3221873)[0m              'experiment_name': 'MATH_baseline',
[36m(main_task pid=3221873)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=3221873)[0m              'n_gpus_per_node': 8,
[36m(main_task pid=3221873)[0m              'nnodes': 1,
[36m(main_task pid=3221873)[0m              'project_name': 'Qwen2.5-3B-Instruct-GRPO',
[36m(main_task pid=3221873)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=3221873)[0m              'resume_from_path': False,
[36m(main_task pid=3221873)[0m              'resume_mode': 'auto',
[36m(main_task pid=3221873)[0m              'save_freq': 10,
[36m(main_task pid=3221873)[0m              'test_freq': 10,
[36m(main_task pid=3221873)[0m              'total_epochs': 2,
[36m(main_task pid=3221873)[0m              'total_training_steps': None,
[36m(main_task pid=3221873)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=3221873)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=3221873)[0m original dataset len: 7500
[36m(main_task pid=3221873)[0m filter dataset len: 7493
[36m(main_task pid=3221873)[0m original dataset len: 5000
[36m(main_task pid=3221873)[0m filter dataset len: 4998
[36m(main_task pid=3221873)[0m Size of train dataloader: 7
[36m(main_task pid=3221873)[0m Size of val dataloader: 1
[36m(main_task pid=3221873)[0m Total training steps: 14
[36m(WorkerDict pid=3222512)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3222512)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3222512)[0m   "architectures": [
[36m(WorkerDict pid=3222512)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3222512)[0m   ],
[36m(WorkerDict pid=3222512)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3222512)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3222512)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3222512)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3222512)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3222512)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3222512)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3222512)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3222512)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3222512)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3222512)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3222512)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3222512)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3222512)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3222512)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3222512)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3222512)[0m   "sliding_window": null,
[36m(WorkerDict pid=3222512)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3222512)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3222512)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3222512)[0m   "use_cache": true,
[36m(WorkerDict pid=3222512)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3222512)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3222512)[0m }
[36m(WorkerDict pid=3222512)[0m 
[36m(WorkerDict pid=3222512)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3222512)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3222512)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f4e29dffec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f4e29dffd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3222512)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3222512)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3222512)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3222512)[0m   "architectures": [
[36m(WorkerDict pid=3222512)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3222512)[0m   ],
[36m(WorkerDict pid=3222512)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3222512)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3222512)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3222512)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3222512)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3222512)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3222512)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3222512)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3222512)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3222512)[0m   "num_attention_heads": 16,[36m(WorkerDict pid=3222512)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(pid=3222780)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 6x across cluster][0m
[36m(pid=3222780)[0m No module named 'vllm._version'[32m [repeated 6x across cluster][0m
[36m(pid=3222780)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3222780)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222780)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222780)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.50it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222780)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.67it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222780)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222779)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3222776)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.32s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3222512)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]
[36m(WorkerDict pid=3222774)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]
[36m(WorkerDict pid=3222512)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3222512)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3222776)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it][32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3222512)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3222512)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=3222512)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3222512)[0m   warnings.warn(
[36m(WorkerDict pid=3222777)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=3222777)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222777)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 7x across cluster][0m
[36m(main_task pid=3221873)[0m wandb: Currently logged in as: zhliu0627 (northwestern_university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=3221873)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=3221873)[0m wandb: Tracking run with wandb version 0.19.5
[36m(main_task pid=3221873)[0m wandb: Run data is saved locally in /wang_ssd/zhihanliu/zhihan/verl/wandb/run-20250223_114418-ghdv9uzr
[36m(main_task pid=3221873)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=3221873)[0m wandb: Syncing run MATH_baseline
[36m(main_task pid=3221873)[0m wandb: ⭐️ View project at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO
[36m(main_task pid=3221873)[0m wandb: 🚀 View run at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO/runs/ghdv9uzr

[36m(WorkerDict pid=3222512)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3222512)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3222512)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3222512)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3222512)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3222512)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3222512)[0m   "sliding_window": null,
[36m(WorkerDict pid=3222512)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3222512)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3222512)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3222512)[0m   "use_cache": true,
[36m(WorkerDict pid=3222512)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3222512)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3222512)[0m }
[36m(WorkerDict pid=3222512)[0m 
[36m(WorkerDict pid=3222780)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fb5aa56fec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fb5aa56fd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3222512)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3222780)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222775)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fc3a1fd3ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fc3a1fd3d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3222512)[0m Total steps: 14, num_warmup_steps: 0
[36m(WorkerDict pid=3222512)[0m Before building vllm rollout, memory allocated (GB): 1.4370465278625488, memory reserved (GB): 5.68359375
[36m(WorkerDict pid=3222512)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3222512)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f4e29dffec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f4e29dffd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222775)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3222512)[0m INFO 02-23 11:44:07 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3222512)[0m INFO 02-23 11:44:07 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=3222512)[0m WARNING 02-23 11:44:07 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3222512)[0m local rank 0
[36m(WorkerDict pid=3222512)[0m INFO 02-23 11:44:08 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3222780)[0m Total steps: 14, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222780)[0m Actor use_remove_padding=True[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3222512)[0m INFO 02-23 11:44:09 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3222512)[0m INFO 02-23 11:44:09 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3222775)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3222775)[0m INFO 02-23 11:44:09 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc2d4c02bd0>, local_subscribe_port=52367, remote_subscribe_port=None)
[36m(WorkerDict pid=3222512)[0m before init cache memory allocated: 4.700148736GB, reserved: 4.79199232GB
[36m(WorkerDict pid=3222512)[0m after init cache memory allocated: 30.89777152GB, reserved: 30.989615104GB
[36m(WorkerDict pid=3222777)[0m INFO 02-23 11:44:08 config.py:887] Defaulting to use ray for distributed inference[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222777)[0m INFO 02-23 11:44:08 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222777)[0m WARNING 02-23 11:44:08 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222777)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222780)[0m INFO 02-23 11:44:10 selector.py:115] Using XFormers backend.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3222780)[0m INFO 02-23 11:44:09 utils.py:1008] Found nccl from library libnccl.so.2[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222780)[0m INFO 02-23 11:44:09 pynccl.py:63] vLLM is using nccl==2.20.5[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222777)[0m NCCL version 2.20.5+cuda12.4[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3222779)[0m INFO 02-23 11:44:09 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f231426edd0>, local_subscribe_port=53503, remote_subscribe_port=None)[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3222512)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3222512)[0m After building vllm rollout, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(WorkerDict pid=3222512)[0m After building sharding manager, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(main_task pid=3221873)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(main_task pid=3221873)[0m Checkpoint tracker file does not exist: %s /wang_ssd/zhihanliu/zhihan/verl/checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH_baseline/latest_checkpointed_iteration.txt
[36m(main_task pid=3221873)[0m Training from scratch
[36m(main_task pid=3221873)[0m validation generation end
[36m(WorkerDict pid=3222778)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
[36m(main_task pid=3221873)[0m <|im_start|>system
[36m(main_task pid=3221873)[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
[36m(main_task pid=3221873)[0m <|im_start|>user
[36m(main_task pid=3221873)[0m Two different numbers are randomly selected from the set $\{1, 2, 3, 4\}$ and they are multiplied. What is the probability that the product is even? Express your answer as a common fraction. Let's think step by step and output the final answer within \boxed{}.<|im_end|>
[36m(main_task pid=3221873)[0m <|im_start|>assistant
[36m(main_task pid=3221873)[0m To determine the probability that the product of two different numbers randomly selected from the set \(\{1, 2, 3, 4\}\) is even, we can follow these steps:
[36m(main_task pid=3221873)[0m 
[36m(main_task pid=3221873)[0m 1. **Identify the total number of possible outcomes:**
[36m(main_task pid=3221873)[0m    The set \(\{1, 2, 3, 4\}\) has 4 elements. We need to select 2 different numbers from this set. The number of ways to choose 2 different numbers from 4 is given by the combination formula \(\binom{4}{2}\):
[36m(main_task pid=3221873)[0m    \[
[36m(main_task pid=3221873)[0m    \binom{4}{2} = \frac{4!}{2!2!} = 6
[36m(main_task pid=3221873)[0m    \]
[36m(main_task pid=3221873)[0m    So, there are 6 possible pairs of numbers that can be selected.
[36m(main_task pid=3221873)[0m 
[36m(main_task pid=3221873)[0m 2. **List all possible pairs and their products:**[36m(WorkerDict pid=3222775)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3222775)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3222778)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222778)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3222778)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.[32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3222778)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3222774)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3222774)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3222777)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3222777)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3222776)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3222776)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3222512)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3222512)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3222775)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3222775)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]

[36m(main_task pid=3221873)[0m    The possible pairs and their products are:
[36m(main_task pid=3221873)[0m    - \((1, 2) \rightarrow 1 \times 2 = 2\)
[36m(main_task pid=3221873)[0m    - \((1, 3) \rightarrow 1 \times 3 = 3\)
[36m(main_task pid=3221873)[0m    - \((1, 4) \rightarrow 1 \times 4 = 4\)
[36m(main_task pid=3221873)[0m    - \((2, 3) \rightarrow 2 \times 3 = 6\)
[36m(main_task pid=3221873)[0m    - \((2, 4) \rightarrow 2 \times 4 = 8\)
[36m(main_task pid=3221873)[0m    - \((3, 4) \rightarrow 3 \times 4 = 12\)
[36m(main_task pid=3221873)[0m    So, the products are \(2, 3, 4, 6, 8, 12\).
[36m(main_task pid=3221873)[0m 
[36m(main_task pid=3221873)[0m 3. **Identify the pairs that result in an even product:**
[36m(main_task pid=3221873)[0m    An even product is one that is divisible by 2. From the list of products, the even products are \(2, 4, 6, 8, 12\). The pairs that give these products are:
[36m(main_task pid=3221873)[0m    - \((1, 2) \rightarrow 2\)
[36m(main_task pid=3221873)[0m    - \((1, 4) \rightarrow 4\)
[36m(main_task pid=3221873)[0m    - \((2, 3) \rightarrow 6\)
[36m(main_task pid=3221873)[0m    - \((2, 4) \rightarrow 8\)
[36m(main_task pid=3221873)[0m    - \((3, 4) \rightarrow 12\)
[36m(main_task pid=3221873)[0m    So, there are 5 pairs that result in an even product.
[36m(main_task pid=3221873)[0m 
[36m(main_task pid=3221873)[0m 4. **Calculate the probability:**
[36m(main_task pid=3221873)[0m    The probability that the product is even is the number of favorable outcomes divided by the total number of possible outcomes. Here, the number of favorable outcomes is 5 and the total number of possible outcomes is 6. Therefore, the probability is:
[36m(main_task pid=3221873)[0m    \[
[36m(main_task pid=3221873)[0m    \frac{5}{6}
[36m(main_task pid=3221873)[0m    \]
[36m(main_task pid=3221873)[0m 
[36m(main_task pid=3221873)[0m The final answer is \(\boxed{\frac{5}{6}}\).<|im_end|>
[36m(main_task pid=3221873)[0m ('Initial validation metrics: '
[36m(main_task pid=3221873)[0m  "{'val/test_score/DigitalLearningGmbH/MATH-lighteval': 0.6360544217687075}")
[36m(main_task pid=3221873)[0m step:0 - val/test_score/DigitalLearningGmbH/MATH-lighteval:0.636
[36m(main_task pid=3221873)[0m step:1 - global_seqlen/min:414907.000 - global_seqlen/max:462017.000 - global_seqlen/minmax_diff:47110.000 - global_seqlen/balanced_min:439742.000 - global_seqlen/balanced_max:439743.000 - global_seqlen/mean:439742.250 - actor/kl_loss:0.000 - actor/kl_coef:0.001 - actor/entropy_loss:0.167 - actor/pg_loss:0.039 - actor/pg_clipfrac:0.000 - actor/ppo_kl:0.000 - actor/grad_norm:0.088 - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.652 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.652 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.013 - critic/advantages/max:1.789 - critic/advantages/min:-1.789 - critic/returns/mean:-0.013 - critic/returns/max:1.789 - critic/returns/min:-1.789 - response_length/mean:568.510 - response_length/max:2048.000 - response_length/min:97.000 - response_length/clip_ratio:0.004 - prompt_length/mean:118.587 - prompt_length/max:1004.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:626.723 - timing_s/old_log_prob:183.026 - timing_s/ref:184.010 - timing_s/adv:1.953 - timing_s/update_actor:707.256 - timing_s/step:1703.433 - timing_per_token_ms/ref:0.052 - timing_per_token_ms/update_actor:0.201 - timing_per_token_ms/gen:0.215 - timing_per_token_ms/adv:0.001
[36m(main_task pid=3221873)[0m step:2 - global_seqlen/min:423120.000 - global_seqlen/max:464140.000 - global_seqlen/minmax_diff:41020.000 - global_seqlen/balanced_min:436801.000 - global_seqlen/balanced_max:436802.000 - global_seqlen/mean:436801.125 - actor/kl_loss:0.000 - actor/kl_coef:0.001 - actor/entropy_loss:0.172 - actor/pg_loss:0.028 - actor/pg_clipfrac:0.000 - actor/ppo_kl:-0.000 - actor/grad_norm:0.085 - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.640 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.640 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.013 - critic/advantages/max:1.789 - critic/advantages/min:-1.789 - critic/returns/mean:-0.013 - critic/returns/max:1.789 - critic/returns/min:-1.789 - response_length/mean:561.358 - response_length/max:2048.000 - response_length/min:92.000 - response_length/clip_ratio:0.003 - prompt_length/mean:121.144 - prompt_length/max:906.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:642.138 - timing_s/old_log_prob:181.968 - timing_s/ref:181.731 - timing_s/adv:1.974 - timing_s/update_actor:701.612 - timing_s/step:1709.745 - timing_per_token_ms/ref:0.052 - timing_per_token_ms/update_actor:0.201 - timing_per_token_ms/gen:0.223 - timing_per_token_ms/adv:0.001
[36m(main_task pid=3221873)[0m step:3 - global_seqlen/min:414272.000 - global_seqlen/max:476536.000 - global_seqlen/minmax_diff:62264.000 - global_seqlen/balanced_min:448766.000 - global_seqlen/balanced_max:448767.000 - global_seqlen/mean:448766.125 - actor/kl_loss:0.000 - actor/kl_coef:0.001 - actor/entropy_loss:0.162 - actor/pg_loss:0.047 - actor/pg_clipfrac:0.000 - actor/ppo_kl:-0.000 - actor/grad_norm:0.079 - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.652 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.652 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.017 - critic/advantages/max:1.789 - critic/advantages/min:-1.789 - critic/returns/mean:-0.017 - critic/returns/max:1.789 - critic/returns/min:-1.789 - response_length/mean:586.289 - response_length/max:2048.000 - response_length/min:90.000 - response_length/clip_ratio:0.007 - prompt_length/mean:114.908 - prompt_length/max:858.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:662.740 - timing_s/old_log_prob:188.926 - timing_s/ref:192.318 - timing_s/adv:2.255 - timing_s/update_actor:741.743 - timing_s/step:1788.310 - timing_per_token_ms/ref:0.054 - timing_per_token_ms/update_actor:0.207 - timing_per_token_ms/gen:0.221 - timing_per_token_ms/adv:0.001
[36m(main_task pid=3221873)[0m step:4 - global_seqlen/min:419425.000 - global_seqlen/max:486372.000 - global_seqlen/minmax_diff:66947.000 - global_seqlen/balanced_min:454422.000 - global_seqlen/balanced_max:454423.000 - global_seqlen/mean:454422.250 - actor/kl_loss:0.000 - actor/kl_coef:0.001 - actor/entropy_loss:0.169 - actor/pg_loss:-0.012 - actor/pg_clipfrac:0.000 - actor/ppo_kl:0.000 - actor/grad_norm:0.083 - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.629 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.629 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.014 - critic/advantages/max:1.789 - critic/advantages/min:-1.789 - critic/returns/mean:-0.014 - critic/returns/max:1.789 - critic/returns/min:-1.789 - response_length/mean:591.675 - response_length/max:2048.000 - response_length/min:107.000 - response_length/clip_ratio:0.003 - prompt_length/mean:118.359 - prompt_length/max:884.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:674.823 - timing_s/old_log_prob:194.322 - timing_s/ref:193.552 - timing_s/adv:2.049 - timing_s/update_actor:756.871 - timing_s/step:1821.959 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/update_actor:0.208 - timing_per_token_ms/gen:0.223 - timing_per_token_ms/adv:0.001
[36m(main_task pid=3221873)[0m step:5 - global_seqlen/min:430211.000 - global_seqlen/max:456206.000 - global_seqlen/minmax_diff:25995.000 - global_seqlen/balanced_min:446937.000 - global_seqlen/balanced_max:446938.000 - global_seqlen/mean:446937.125 - actor/kl_loss:0.001 - actor/kl_coef:0.001 - actor/entropy_loss:0.154 - actor/pg_loss:0.012 - actor/pg_clipfrac:0.000 - actor/ppo_kl:0.000 - actor/grad_norm:0.085 - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.641 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.641 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.014 - critic/advantages/max:1.789 - critic/advantages/min:-1.789 - critic/returns/mean:-0.014 - critic/returns/max:1.789 - critic/returns/min:-1.789 - response_length/mean:581.068 - response_length/max:2048.000 - response_length/min:114.000 - response_length/clip_ratio:0.004 - prompt_length/mean:117.271 - prompt_length/max:882.000 - prompt_length/min:51.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:662.491 - timing_s/old_log_prob:188.335 - timing_s/ref:190.983 - timing_s/adv:1.986 - timing_s/update_actor:743.451 - timing_s/step:1787.531 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/update_actor:0.208 - timing_per_token_ms/gen:0.223 - timing_per_token_ms/adv:0.001[36m(main_task pid=3221873)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=3222777, ip=10.14.38.55, actor_id=54a625465650ac3aa9d5d31201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc155b66590>)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 399, in func
[36m(main_task pid=3221873)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/base/decorator.py", line 404, in inner
[36m(main_task pid=3221873)[0m     return func(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/fsdp_workers.py", line 517, in compute_log_prob
[36m(main_task pid=3221873)[0m     output = self.actor.compute_log_prob(data=data)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 191, in compute_log_prob
[36m(main_task pid=3221873)[0m     _, log_probs = self._forward_micro_batch(micro_batch, temperature=temperature)
[36m(main_task pid=3221873)[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 94, in _forward_micro_batch
[36m(main_task pid=3221873)[0m     output = self.actor_module(input_ids=input_ids_rmpad,
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[36m(main_task pid=3221873)[0m     output = self._fsdp_wrapped_module(*args, **kwargs)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1180, in forward
[36m(main_task pid=3221873)[0m     logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 117, in forward
[36m(main_task pid=3221873)[0m     return F.linear(input, self.weight, self.bias)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.47 GiB. GPU 0 has a total capacity of 47.43 GiB of which 34.76 GiB is free. Including non-PyTorch memory, this process has 12.64 GiB memory in use. Of the allocated memory 5.51 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(main_task pid=3221873)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=3222776, ip=10.14.38.55, actor_id=ae585dde5cd64a4c0e2bf06701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f9900423750>)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 399, in func
[36m(main_task pid=3221873)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/base/decorator.py", line 404, in inner
[36m(main_task pid=3221873)[0m     return func(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/fsdp_workers.py", line 517, in compute_log_prob
[36m(main_task pid=3221873)[0m     output = self.actor.compute_log_prob(data=data)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 191, in compute_log_prob
[36m(main_task pid=3221873)[0m     _, log_probs = self._forward_micro_batch(micro_batch, temperature=temperature)
[36m(main_task pid=3221873)[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 94, in _forward_micro_batch
[36m(main_task pid=3221873)[0m     output = self.actor_module(input_ids=input_ids_rmpad,
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[36m(main_task pid=3221873)[0m     output = self._fsdp_wrapped_module(*args, **kwargs)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1180, in forward
[36m(main_task pid=3221873)[0m     logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 117, in forward
[36m(main_task pid=3221873)[0m     return F.linear(input, self.weight, self.bias)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.53 GiB. GPU 0 has a total capacity of 47.43 GiB of which 38.90 GiB is free. Including non-PyTorch memory, this process has 8.50 GiB memory in use. Of the allocated memory 5.51 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(main_task pid=3221873)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2310: UserWarning: Run (ghdv9uzr) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
[36m(main_task pid=3221873)[0m   lambda data: self._console_raw_callback("stderr", data),
[36m(main_task pid=3221873)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=3222775, ip=10.14.38.55, actor_id=68f6d6988b6f7384f3d49e4f01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc382b16e50>)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 399, in func
[36m(main_task pid=3221873)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/base/decorator.py", line 404, in inner
[36m(main_task pid=3221873)[0m     return func(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/fsdp_workers.py", line 517, in compute_log_prob
[36m(main_task pid=3221873)[0m     output = self.actor.compute_log_prob(data=data)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 191, in compute_log_prob
[36m(main_task pid=3221873)[0m     _, log_probs = self._forward_micro_batch(micro_batch, temperature=temperature)
[36m(main_task pid=3221873)[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 94, in _forward_micro_batch
[36m(main_task pid=3221873)[0m     output = self.actor_module(input_ids=input_ids_rmpad,
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[36m(main_task pid=3221873)[0m     output = self._fsdp_wrapped_module(*args, **kwargs)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1180, in forward
[36m(main_task pid=3221873)[0m     logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 117, in forward
[36m(main_task pid=3221873)[0m     return F.linear(input, self.weight, self.bias)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.26 GiB. GPU 0 has a total capacity of 47.43 GiB of which 38.86 GiB is free. Including non-PyTorch memory, this process has 8.55 GiB memory in use. Of the allocated memory 5.51 GiB is allocated by PyTorch, and 2.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(main_task pid=3221873)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=3222512, ip=10.14.38.55, actor_id=d446b2d59aa979e0ad8bd11e01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f4e0b0c0210>)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 399, in func
[36m(main_task pid=3221873)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/base/decorator.py", line 404, in inner
[36m(main_task pid=3221873)[0m     return func(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/fsdp_workers.py", line 517, in compute_log_prob
[36m(main_task pid=3221873)[0m     output = self.actor.compute_log_prob(data=data)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 191, in compute_log_prob
[36m(main_task pid=3221873)[0m     _, log_probs = self._forward_micro_batch(micro_batch, temperature=temperature)
[36m(main_task pid=3221873)[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 94, in _forward_micro_batch
[36m(main_task pid=3221873)[0m     output = self.actor_module(input_ids=input_ids_rmpad,
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[36m(main_task pid=3221873)[0m     output = self._fsdp_wrapped_module(*args, **kwargs)
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1180, in forward
[36m(main_task pid=3221873)[0m     logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
[36m(main_task pid=3221873)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=3221873)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=3221873)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m   File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 117, in forward
[36m(main_task pid=3221873)[0m     return F.linear(input, self.weight, self.bias)
[36m(main_task pid=3221873)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=3221873)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.98 GiB. GPU 0 has a total capacity of 47.43 GiB of which 38.90 GiB is free. Including non-PyTorch memory, this process has 8.50 GiB memory in use. Of the allocated memory 5.50 GiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(main_task pid=3221873)[0m wandb:                                                                                
[36m(main_task pid=3221873)[0m wandb: 
[36m(main_task pid=3221873)[0m wandb: Run history:
[36m(main_task pid=3221873)[0m wandb:                                actor/entropy_loss ▆█▄▇▁ 
[36m(main_task pid=3221873)[0m wandb:                                   actor/grad_norm █▆▁▄▆ 
[36m(main_task pid=3221873)[0m wandb:                                     actor/kl_coef ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                                     actor/kl_loss ▂▃▁▄█ 
[36m(main_task pid=3221873)[0m wandb:                                          actor/lr ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                                 actor/pg_clipfrac ▁▁▁▁▁█
[36m(main_task pid=3221873)[0m wandb:                                     actor/pg_loss ▇▆█▁▄ 
[36m(main_task pid=3221873)[0m wandb:                                      actor/ppo_kl ▆▂▁█▄ 
[36m(main_task pid=3221873)[0m wandb:                             critic/advantages/max ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                            critic/advantages/mean ▇█▁▆▆▃
[36m(main_task pid=3221873)[0m wandb:                             critic/advantages/min ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                                critic/returns/max ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                               critic/returns/mean ▇█▁▆▆▃
[36m(main_task pid=3221873)[0m wandb:                                critic/returns/min ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                                critic/rewards/max ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                               critic/rewards/mean █▄█▁▅▄
[36m(main_task pid=3221873)[0m wandb:                                critic/rewards/min ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                                  critic/score/max ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                                 critic/score/mean █▄█▁▅▄
[36m(main_task pid=3221873)[0m wandb:                                  critic/score/min ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                        global_seqlen/balanced_max ▂▁▆█▅▆
[36m(main_task pid=3221873)[0m wandb:                        global_seqlen/balanced_min ▂▁▆█▅▆
[36m(main_task pid=3221873)[0m wandb:                                 global_seqlen/max ▂▃▆█▁█
[36m(main_task pid=3221873)[0m wandb:                                global_seqlen/mean ▂▁▆█▅▆
[36m(main_task pid=3221873)[0m wandb:                                 global_seqlen/min ▁▅▁▃█▅
[36m(main_task pid=3221873)[0m wandb:                         global_seqlen/minmax_diff ▅▄▇█▁▇
[36m(main_task pid=3221873)[0m wandb:                                         mfu/actor ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                          prompt_length/clip_ratio ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                                 prompt_length/max █▃▁▂▂▅
[36m(main_task pid=3221873)[0m wandb:                                prompt_length/mean ▅█▁▅▄▆
[36m(main_task pid=3221873)[0m wandb:                                 prompt_length/min ████▁█
[36m(main_task pid=3221873)[0m wandb:                        response_length/clip_ratio ▃▁█▁▃▅
[36m(main_task pid=3221873)[0m wandb:                               response_length/max ▁▁▁▁▁▁
[36m(main_task pid=3221873)[0m wandb:                              response_length/mean ▃▁▇█▆▆
[36m(main_task pid=3221873)[0m wandb:                               response_length/min ▄▃▂▆█▁
[36m(main_task pid=3221873)[0m wandb:                           timing_per_token_ms/adv ▁▂█▂▁▂
[36m(main_task pid=3221873)[0m wandb:                           timing_per_token_ms/gen ▁▆▄▅▅█
[36m(main_task pid=3221873)[0m wandb:                           timing_per_token_ms/ref ▂▁█▇▇▇
[36m(main_task pid=3221873)[0m wandb:                  timing_per_token_ms/update_actor ▁▁▆██▆
[36m(main_task pid=3221873)[0m wandb:                                      timing_s/adv ▁▂█▃▂▃
[36m(main_task pid=3221873)[0m wandb:                                      timing_s/gen ▁▃▆█▆█
[36m(main_task pid=3221873)[0m wandb:                             timing_s/old_log_prob ▂▁▅█▅▅
[36m(main_task pid=3221873)[0m wandb:                                      timing_s/ref ▂▁▇█▆▇
[36m(main_task pid=3221873)[0m wandb:                                     timing_s/step ▁▁▆█▆▇
[36m(main_task pid=3221873)[0m wandb:                             timing_s/update_actor ▂▁▆█▆▆
[36m(main_task pid=3221873)[0m wandb: val/test_score/DigitalLearningGmbH/MATH-lighteval ▁
[36m(main_task pid=3221873)[0m wandb: 
[36m(main_task pid=3221873)[0m wandb: Run summary:
[36m(main_task pid=3221873)[0m wandb:                                actor/entropy_loss nan
[36m(main_task pid=3221873)[0m wandb:                                   actor/grad_norm nan
[36m(main_task pid=3221873)[0m wandb:                                     actor/kl_coef 0.001
[36m(main_task pid=3221873)[0m wandb:                                     actor/kl_loss nan
[36m(main_task pid=3221873)[0m wandb:                                          actor/lr 0.0
[36m(main_task pid=3221873)[0m wandb:                                 actor/pg_clipfrac 0.09001
[36m(main_task pid=3221873)[0m wandb:                                     actor/pg_loss nan
[36m(main_task pid=3221873)[0m wandb:                                      actor/ppo_kl nan
[36m(main_task pid=3221873)[0m wandb:                             critic/advantages/max 1.78885
[36m(main_task pid=3221873)[0m wandb:                            critic/advantages/mean -0.01603
[36m(main_task pid=3221873)[0m wandb:                             critic/advantages/min -1.78885
[36m(main_task pid=3221873)[0m wandb:                                critic/returns/max 1.78885
[36m(main_task pid=3221873)[0m wandb:                               critic/returns/mean -0.01603
[36m(main_task pid=3221873)[0m wandb:                                critic/returns/min -1.78885
[36m(main_task pid=3221873)[0m wandb:                                critic/rewards/max 1
[36m(main_task pid=3221873)[0m wandb:                               critic/rewards/mean 0.63789
[36m(main_task pid=3221873)[0m wandb:                                critic/rewards/min 0
[36m(main_task pid=3221873)[0m wandb:                                  critic/score/max 1
[36m(main_task pid=3221873)[0m wandb:                                 critic/score/mean 0.63789
[36m(main_task pid=3221873)[0m wandb:                                  critic/score/min 0
[36m(main_task pid=3221873)[0m wandb:                        global_seqlen/balanced_max 448257
[36m(main_task pid=3221873)[0m wandb:                        global_seqlen/balanced_min 448256
[36m(main_task pid=3221873)[0m wandb:                                 global_seqlen/max 486028
[36m(main_task pid=3221873)[0m wandb:                                global_seqlen/mean 448256.625
[36m(main_task pid=3221873)[0m wandb:                                 global_seqlen/min 422936
[36m(main_task pid=3221873)[0m wandb:                         global_seqlen/minmax_diff 63092
[36m(main_task pid=3221873)[0m wandb:                                         mfu/actor 0
[36m(main_task pid=3221873)[0m wandb:                          prompt_length/clip_ratio 0
[36m(main_task pid=3221873)[0m wandb:                                 prompt_length/max 947
[36m(main_task pid=3221873)[0m wandb:                                prompt_length/mean 119.00391
[36m(main_task pid=3221873)[0m wandb:                                 prompt_length/min 52
[36m(main_task pid=3221873)[0m wandb:                        response_length/clip_ratio 0.00527
[36m(main_task pid=3221873)[0m wandb:                               response_length/max 2048
[36m(main_task pid=3221873)[0m wandb:                              response_length/mean 581.39709
[36m(main_task pid=3221873)[0m wandb:                               response_length/min 84
[36m(main_task pid=3221873)[0m wandb:                           timing_per_token_ms/adv 0.00056
[36m(main_task pid=3221873)[0m wandb:                           timing_per_token_ms/gen 0.227
[36m(main_task pid=3221873)[0m wandb:                           timing_per_token_ms/ref 0.05328
[36m(main_task pid=3221873)[0m wandb:                  timing_per_token_ms/update_actor 0.20627
[36m(main_task pid=3221873)[0m wandb:                                      timing_s/adv 2.02365
[36m(main_task pid=3221873)[0m wandb:                                      timing_s/gen 675.72575
[36m(main_task pid=3221873)[0m wandb:                             timing_s/old_log_prob 189.84478
[36m(main_task pid=3221873)[0m wandb:                                      timing_s/ref 191.07552
[36m(main_task pid=3221873)[0m wandb:                                     timing_s/step 1798.67801
[36m(main_task pid=3221873)[0m wandb:                             timing_s/update_actor 739.70634
[36m(main_task pid=3221873)[0m wandb: val/test_score/DigitalLearningGmbH/MATH-lighteval 0.63605
[36m(main_task pid=3221873)[0m wandb: 
[36m(main_task pid=3221873)[0m wandb: 🚀 View run MATH_baseline at: https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO/runs/ghdv9uzr
[36m(main_task pid=3221873)[0m wandb: ⭐️ View project at: https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO
[36m(main_task pid=3221873)[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[36m(main_task pid=3221873)[0m wandb: Find logs at: ./wandb/run-20250223_114418-ghdv9uzr/logs

[36m(main_task pid=3221873)[0m step:6 - global_seqlen/min:422936.000 - global_seqlen/max:486028.000 - global_seqlen/minmax_diff:63092.000 - global_seqlen/balanced_min:448256.000 - global_seqlen/balanced_max:448257.000 - global_seqlen/mean:448256.625 - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.090 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.638 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.638 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.016 - critic/advantages/max:1.789 - critic/advantages/min:-1.789 - critic/returns/mean:-0.016 - critic/returns/max:1.789 - critic/returns/min:-1.789 - response_length/mean:581.397 - response_length/max:2048.000 - response_length/min:84.000 - response_length/clip_ratio:0.005 - prompt_length/mean:119.004 - prompt_length/max:947.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:675.726 - timing_s/old_log_prob:189.845 - timing_s/ref:191.076 - timing_s/adv:2.024 - timing_s/update_actor:739.706 - timing_s/step:1798.678 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/update_actor:0.206 - timing_per_token_ms/gen:0.227 - timing_per_token_ms/adv:0.001
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/wang_ssd/zhihanliu/data//train.parquet', 'data.val_files=/wang_ssd/zhihanliu/data//test.parquet', 'data.custom_temp_dir=/wang_ssd/zhihanliu/tmp/ray/', 'data.train_batch_size=1024', 'data.val_batch_size=1024', 'data.max_prompt_length=1024', 'data.max_response_length=2048', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=256', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=64', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=5', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=64', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=Qwen2.5-3B-Instruct-GRPO', 'trainer.experiment_name=MATH_baseline', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=10', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 240, in <module>
    main()
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 128, in main
    run_ppo(config)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 142, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::main_task()[39m (pid=3221873, ip=10.14.38.55)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 236, in main_task
    trainer.fit()
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/ppo/ray_trainer.py", line 913, in fit
    old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=3222774, ip=10.14.38.55, actor_id=8bd75603b37f32c72c72b9f301000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fdf62758a90>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/fsdp_workers.py", line 517, in compute_log_prob
    output = self.actor.compute_log_prob(data=data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 191, in compute_log_prob
    _, log_probs = self._forward_micro_batch(micro_batch, temperature=temperature)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/workers/actor/dp_actor.py", line 94, in _forward_micro_batch
    output = self.actor_module(input_ids=input_ids_rmpad,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1180, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.19 GiB. GPU 0 has a total capacity of 47.43 GiB of which 37.62 GiB is free. Including non-PyTorch memory, this process has 9.78 GiB memory in use. Of the allocated memory 5.51 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Loading the DigitalLearningGmbH/MATH-lighteval dataset from huggingface...
Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 244.08ba/s]
Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 393.29ba/s]
2025-02-23 15:41:32,130	INFO worker.py:1841 -- Started a local Ray instance.
[36m(main_task pid=3300064)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=3300064)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=3300064)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3300064)[0m                                                  'grad_offload': False,
[36m(main_task pid=3300064)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=3300064)[0m                                                  'param_offload': False,
[36m(main_task pid=3300064)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3300064)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=3300064)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=3300064)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=3300064)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=3300064)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3300064)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=3300064)[0m                                            'total_training_steps': -1,
[36m(main_task pid=3300064)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=3300064)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=3300064)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=3300064)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=3300064)[0m                                  'ppo_micro_batch_size_per_gpu': 16,
[36m(main_task pid=3300064)[0m                                  'ppo_mini_batch_size': 256,
[36m(main_task pid=3300064)[0m                                  'shuffle': False,
[36m(main_task pid=3300064)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=3300064)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3300064)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=3300064)[0m                                  'use_kl_loss': True},
[36m(main_task pid=3300064)[0m                        'hybrid_engine': True,
[36m(main_task pid=3300064)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3300064)[0m                                  'external_lib': None,
[36m(main_task pid=3300064)[0m                                  'override_config': {},
[36m(main_task pid=3300064)[0m                                  'path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3300064)[0m                                  'use_remove_padding': True},
[36m(main_task pid=3300064)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=3300064)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3300064)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3300064)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=3300064)[0m                                'log_prob_micro_batch_size_per_gpu': 64,
[36m(main_task pid=3300064)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3300064)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=3300064)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=3300064)[0m                                    'do_sample': True,
[36m(main_task pid=3300064)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=3300064)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=3300064)[0m                                    'enforce_eager': True,
[36m(main_task pid=3300064)[0m                                    'free_cache_engine': True,
[36m(main_task pid=3300064)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=3300064)[0m                                    'ignore_eos': False,
[36m(main_task pid=3300064)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=3300064)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3300064)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=3300064)[0m                                    'log_prob_micro_batch_size_per_gpu': 64,
[36m(main_task pid=3300064)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3300064)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=3300064)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=3300064)[0m                                    'n': 5,
[36m(main_task pid=3300064)[0m                                    'name': 'vllm',
[36m(main_task pid=3300064)[0m                                    'prompt_length': 1024,
[36m(main_task pid=3300064)[0m                                    'response_length': 2048,
[36m(main_task pid=3300064)[0m                                    'temperature': 1.0,
[36m(main_task pid=3300064)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=3300064)[0m                                    'top_k': -1,
[36m(main_task pid=3300064)[0m                                    'top_p': 1}},
[36m(main_task pid=3300064)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=3300064)[0m                'gamma': 1.0,
[36m(main_task pid=3300064)[0m                'kl_ctrl': {'kl_coef': 0.001,
[36m(main_task pid=3300064)[0m                            'kl_coef_correction': 0.01,
[36m(main_task pid=3300064)[0m                            'type': 'fixed'},
[36m(main_task pid=3300064)[0m                'kl_penalty': 'kl',
[36m(main_task pid=3300064)[0m                'lam': 1.0,
[36m(main_task pid=3300064)[0m                'optimism_coef': 0.01,
[36m(main_task pid=3300064)[0m                'optimistic_actor': True,
[36m(main_task pid=3300064)[0m                'optimistic_critic': False},
[36m(main_task pid=3300064)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=3300064)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3300064)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=3300064)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=3300064)[0m             'grad_clip': 1.0,
[36m(main_task pid=3300064)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3300064)[0m                       'external_lib': None,
[36m(main_task pid=3300064)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3300064)[0m                                       'grad_offload': False,
[36m(main_task pid=3300064)[0m                                       'optimizer_offload': False,
[36m(main_task pid=3300064)[0m                                       'param_offload': False,
[36m(main_task pid=3300064)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3300064)[0m                       'override_config': {},
[36m(main_task pid=3300064)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=3300064)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3300064)[0m                       'use_remove_padding': False},
[36m(main_task pid=3300064)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=3300064)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3300064)[0m                       'min_lr_ratio': None,[36m(main_task pid=3300064)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=3300064)[0m No module named 'vllm._version'
[36m(main_task pid=3300064)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3300556)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3300556)[0m No module named 'vllm._version'
[36m(pid=3300556)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3300756)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3300756)[0m No module named 'vllm._version'
[36m(pid=3300756)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=3300756)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3300756)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3300756)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.64it/s]
[36m(WorkerDict pid=3300756)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.87it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.83it/s]
[36m(WorkerDict pid=3300753)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(pid=3300754)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=3300754)[0m No module named 'vllm._version'[32m [repeated 6x across cluster][0m
[36m(pid=3300754)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3300757)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300757)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300757)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.86it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300757)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.12it/s][32m [repeated 7x across cluster][0m

[36m(main_task pid=3300064)[0m                       'total_training_steps': -1,
[36m(main_task pid=3300064)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=3300064)[0m             'ppo_epochs': 1,
[36m(main_task pid=3300064)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=3300064)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=3300064)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=3300064)[0m             'ppo_mini_batch_size': 256,
[36m(main_task pid=3300064)[0m             'shuffle': False,
[36m(main_task pid=3300064)[0m             'strategy': 'fsdp',
[36m(main_task pid=3300064)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3300064)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=3300064)[0m  'data': {'custom_temp_dir': '/wang_ssd/zhihanliu/tmp/ray/',
[36m(main_task pid=3300064)[0m           'max_prompt_length': 1024,
[36m(main_task pid=3300064)[0m           'max_response_length': 2048,
[36m(main_task pid=3300064)[0m           'prompt_key': 'prompt',
[36m(main_task pid=3300064)[0m           'return_raw_chat': False,
[36m(main_task pid=3300064)[0m           'return_raw_input_ids': False,
[36m(main_task pid=3300064)[0m           'save_ppo_rollouts_path': '',
[36m(main_task pid=3300064)[0m           'shuffle': True,
[36m(main_task pid=3300064)[0m           'tokenizer': None,
[36m(main_task pid=3300064)[0m           'train_batch_size': 1024,
[36m(main_task pid=3300064)[0m           'train_files': '/wang_ssd/zhihanliu/data//train.parquet',
[36m(main_task pid=3300064)[0m           'val_batch_size': 1024,
[36m(main_task pid=3300064)[0m           'val_files': '/wang_ssd/zhihanliu/data//test.parquet'},
[36m(main_task pid=3300064)[0m  'reward_model': {'enable': False,
[36m(main_task pid=3300064)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3300064)[0m                   'max_length': None,
[36m(main_task pid=3300064)[0m                   'micro_batch_size': None,
[36m(main_task pid=3300064)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=3300064)[0m                   'model': {'external_lib': None,
[36m(main_task pid=3300064)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3300064)[0m                                             'min_num_params': 0,
[36m(main_task pid=3300064)[0m                                             'param_offload': False},
[36m(main_task pid=3300064)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3300064)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=3300064)[0m                             'use_remove_padding': False},
[36m(main_task pid=3300064)[0m                   'reward_manager': 'naive',
[36m(main_task pid=3300064)[0m                   'strategy': 'fsdp',
[36m(main_task pid=3300064)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3300064)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=3300064)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=3300064)[0m              'default_hdfs_dir': None,
[36m(main_task pid=3300064)[0m              'default_local_dir': 'checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH-mex0.01',
[36m(main_task pid=3300064)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=3300064)[0m              'experiment_name': 'MATH-mex0.01',
[36m(main_task pid=3300064)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=3300064)[0m              'n_gpus_per_node': 8,
[36m(main_task pid=3300064)[0m              'nnodes': 1,
[36m(main_task pid=3300064)[0m              'project_name': 'Qwen2.5-3B-Instruct-GRPO',
[36m(main_task pid=3300064)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=3300064)[0m              'resume_from_path': False,
[36m(main_task pid=3300064)[0m              'resume_mode': 'auto',
[36m(main_task pid=3300064)[0m              'save_freq': 10,
[36m(main_task pid=3300064)[0m              'test_freq': 10,
[36m(main_task pid=3300064)[0m              'total_epochs': 2,
[36m(main_task pid=3300064)[0m              'total_training_steps': None,
[36m(main_task pid=3300064)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=3300064)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=3300064)[0m original dataset len: 7500
[36m(main_task pid=3300064)[0m filter dataset len: 7493
[36m(main_task pid=3300064)[0m original dataset len: 5000
[36m(main_task pid=3300064)[0m filter dataset len: 4998
[36m(main_task pid=3300064)[0m Size of train dataloader: 7
[36m(main_task pid=3300064)[0m Size of val dataloader: 1
[36m(main_task pid=3300064)[0m Total training steps: 14
[36m(WorkerDict pid=3300556)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3300556)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3300556)[0m   "architectures": [
[36m(WorkerDict pid=3300556)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3300556)[0m   ],
[36m(WorkerDict pid=3300556)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3300556)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3300556)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3300556)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3300556)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3300556)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3300556)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3300556)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3300556)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3300556)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3300556)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3300556)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3300556)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3300556)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3300556)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3300556)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3300556)[0m   "sliding_window": null,
[36m(WorkerDict pid=3300556)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3300556)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3300556)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3300556)[0m   "use_cache": true,
[36m(WorkerDict pid=3300556)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3300556)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3300556)[0m }
[36m(WorkerDict pid=3300556)[0m 
[36m(WorkerDict pid=3300556)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3300556)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3300556)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fc82d797ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fc82d797d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3300556)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3300556)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3300556)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3300556)[0m   "architectures": [
[36m(WorkerDict pid=3300556)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3300556)[0m   ],
[36m(WorkerDict pid=3300556)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3300556)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3300556)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3300556)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3300556)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3300556)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3300556)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3300556)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3300556)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3300556)[0m   "num_attention_heads": 16,[36m(WorkerDict pid=3300755)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300755)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3300755)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.94s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3300756)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]
[36m(WorkerDict pid=3300556)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]
[36m(WorkerDict pid=3300751)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3300751)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3300753)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.28s/it][32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3300751)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3300751)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=3300753)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3300753)[0m   warnings.warn(
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=3300755)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300755)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 7x across cluster][0m
[36m(main_task pid=3300064)[0m wandb: Currently logged in as: zhliu0627 (northwestern_university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=3300064)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=3300064)[0m wandb: Tracking run with wandb version 0.19.5
[36m(main_task pid=3300064)[0m wandb: Run data is saved locally in /wang_ssd/zhihanliu/zhihan/verl/wandb/run-20250223_154234-pkz1qxr1
[36m(main_task pid=3300064)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=3300064)[0m wandb: Syncing run MATH-mex0.01
[36m(main_task pid=3300064)[0m wandb: ⭐️ View project at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO
[36m(main_task pid=3300064)[0m wandb: 🚀 View run at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO/runs/pkz1qxr1

[36m(WorkerDict pid=3300556)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3300556)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3300556)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3300556)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3300556)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3300556)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3300556)[0m   "sliding_window": null,
[36m(WorkerDict pid=3300556)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3300556)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3300556)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3300556)[0m   "use_cache": true,
[36m(WorkerDict pid=3300556)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3300556)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3300556)[0m }
[36m(WorkerDict pid=3300556)[0m 
[36m(WorkerDict pid=3300756)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f412870bec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f412870bd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300556)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3300756)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300556)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fc82d797ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fc82d797d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3300753)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f75c9603ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f75c9603d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3300751)[0m Total steps: 14, num_warmup_steps: 0
[36m(WorkerDict pid=3300751)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3300756)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f412870bec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f412870bd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3300556)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3300556)[0m Before building vllm rollout, memory allocated (GB): 1.4370465278625488, memory reserved (GB): 5.68359375
[36m(WorkerDict pid=3300751)[0m INFO 02-23 15:42:24 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3300751)[0m INFO 02-23 15:42:24 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=3300751)[0m WARNING 02-23 15:42:24 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3300751)[0m local rank 0
[36m(WorkerDict pid=3300751)[0m INFO 02-23 15:42:24 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3300756)[0m Total steps: 14, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300756)[0m Actor use_remove_padding=True[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3300556)[0m INFO 02-23 15:42:25 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3300556)[0m INFO 02-23 15:42:25 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3300752)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3300752)[0m INFO 02-23 15:42:25 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f950a2f6d90>, local_subscribe_port=44099, remote_subscribe_port=None)
[36m(WorkerDict pid=3300556)[0m before init cache memory allocated: 4.700148736GB, reserved: 4.79199232GB
[36m(WorkerDict pid=3300556)[0m after init cache memory allocated: 30.89777152GB, reserved: 30.989615104GB
[36m(WorkerDict pid=3300755)[0m INFO 02-23 15:42:25 config.py:887] Defaulting to use ray for distributed inference[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300755)[0m INFO 02-23 15:42:25 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300755)[0m WARNING 02-23 15:42:25 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300755)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300756)[0m INFO 02-23 15:42:26 selector.py:115] Using XFormers backend.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3300756)[0m INFO 02-23 15:42:25 utils.py:1008] Found nccl from library libnccl.so.2[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300756)[0m INFO 02-23 15:42:25 pynccl.py:63] vLLM is using nccl==2.20.5[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300756)[0m NCCL version 2.20.5+cuda12.4[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3300756)[0m INFO 02-23 15:42:26 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f4090b9ee90>, local_subscribe_port=50303, remote_subscribe_port=None)[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3300753)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3300556)[0m After building vllm rollout, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(WorkerDict pid=3300556)[0m After building sharding manager, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(main_task pid=3300064)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(main_task pid=3300064)[0m Checkpoint tracker file does not exist: %s /wang_ssd/zhihanliu/zhihan/verl/checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH-mex0.01/latest_checkpointed_iteration.txt
[36m(main_task pid=3300064)[0m Training from scratch
[36m(main_task pid=3300064)[0m validation generation end
[36m(WorkerDict pid=3300754)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
[36m(main_task pid=3300064)[0m <|im_start|>system
[36m(main_task pid=3300064)[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
[36m(main_task pid=3300064)[0m <|im_start|>user
[36m(main_task pid=3300064)[0m The sum of two numbers is 25 and their difference is 9. What is their product? Let's think step by step and output the final answer within \boxed{}.<|im_end|>
[36m(main_task pid=3300064)[0m <|im_start|>assistant
[36m(main_task pid=3300064)[0m To solve the problem, we need to find two numbers whose sum is 25 and whose difference is 9. Let's denote these two numbers by \( x \) and \( y \) where \( x > y \).
[36m(main_task pid=3300064)[0m 
[36m(main_task pid=3300064)[0m We can set up the following system of equations based on the given information:
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m x + y = 25
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m x - y = 9
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m 
[36m(main_task pid=3300064)[0m To find the values of \( x \) and \( y \), we can add these two equations together:
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m (x + y) + (x - y) = 25 + 9
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m \[[36m(WorkerDict pid=3300556)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3300556)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300755)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3300753)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.[32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3300753)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3300757)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3300757)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined][32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3300755)[0m [2025-02-23 16:14:32,435 E 3300755 3304078] logging.cc:108: Unhandled exception: N3c105ErrorE. what(): CUDA error: an illegal memory access was encountered
[36m(WorkerDict pid=3300755)[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[36m(WorkerDict pid=3300755)[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[36m(WorkerDict pid=3300755)[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(WorkerDict pid=3300755)[0m 
[36m(WorkerDict pid=3300755)[0m Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
[36m(WorkerDict pid=3300755)[0m frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f84cf949f86 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so)
[36m(WorkerDict pid=3300755)[0m frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f84cf8f8d10 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so)
[36m(WorkerDict pid=3300755)[0m frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f84cfa24f08 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
[36m(WorkerDict pid=3300755)[0m frame #3: <unknown function> + 0x1da06 (0x7f84cf9efa06 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
[36m(WorkerDict pid=3300755)[0m frame #4: <unknown function> + 0x1f783 (0x7f84cf9f1783 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
[36m(WorkerDict pid=3300755)[0m frame #5: <unknown function> + 0x1fac2 (0x7f84cf9f1ac2 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
[36m(WorkerDict pid=3300755)[0m frame #6: <unknown function> + 0x5de5b0 (0x7f84cdfd65b0 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
[36m(WorkerDict pid=3300755)[0m frame #7: <unknown function> + 0x6abdf (0x7f84cf92dbdf in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so)
[36m(WorkerDict pid=3300755)[0m frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x7f84cf926c3b in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so)
[36m(WorkerDict pid=3300755)[0m frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f84cf926de9 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so)
[36m(WorkerDict pid=3300755)[0m frame #10: std::vector<at::Tensor, std::allocator<at::Tensor> >::~vector() + 0x88 (0x7f84cdfd8428 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
[36m(WorkerDict pid=3300755)[0m frame #11: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x121 (0x7f84baabced1 in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
[36m(WorkerDict pid=3300755)[0m frame #12: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x13f (0x7f84baab43cf in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
[36m(WorkerDict pid=3300755)[0m frame #13: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x5c (0x7f84ce268cac in /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
[36m(WorkerDict pid=3300755)[0m frame #14: <unknown function> + 0xdbbf4 (0x7fa0aaa6fbf4 in /wang_ssd/zhihanliu/anaconda3/envs/verl/bin/../lib/libstdc++.so.6)
[36m(WorkerDict pid=3300755)[0m frame #15: <unknown function> + 0x94ac3 (0x7fa0acd5eac3 in /lib/x86_64-linux-gnu/libc.so.6)
[36m(WorkerDict pid=3300755)[0m frame #16: <unknown function> + 0x126850 (0x7fa0acdf0850 in /lib/x86_64-linux-gnu/libc.so.6)
[36m(WorkerDict pid=3300755)[0m 
[36m(WorkerDict pid=3300755)[0m [2025-02-23 16:14:32,481 E 3300755 3304078] logging.cc:115: Stack trace: 
[36m(WorkerDict pid=3300755)[0m  /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_raylet.so(+0x11d889a) [0x7fa0abd9489a] ray::operator<<()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_raylet.so(+0x11dbe32) [0x7fa0abd97e32] ray::TerminateHandler()
[36m(WorkerDict pid=3300755)[0m /wang_ssd/zhihanliu/anaconda3/envs/verl/bin/../lib/libstdc++.so.6(+0xb135a) [0x7fa0aaa4535a] __cxxabiv1::__terminate()
[36m(WorkerDict pid=3300755)[0m /wang_ssd/zhihanliu/anaconda3/envs/verl/bin/../lib/libstdc++.so.6(+0xb03b9) [0x7fa0aaa443b9]
[36m(WorkerDict pid=3300755)[0m /wang_ssd/zhihanliu/anaconda3/envs/verl/bin/../lib/libstdc++.so.6(__gxx_personality_v0+0x87) [0x7fa0aaa44ae7] __gxx_personality_v0
[36m(WorkerDict pid=3300755)[0m /wang_ssd/zhihanliu/anaconda3/envs/verl/bin/../lib/libgcc_s.so.1(+0x111e4) [0x7fa0aa98b1e4] _Unwind_RaiseException_Phase2
[36m(WorkerDict pid=3300755)[0m /wang_ssd/zhihanliu/anaconda3/envs/verl/bin/../lib/libgcc_s.so.1(_Unwind_Resume+0x12e) [0x7fa0aa98bc1e] _Unwind_Resume
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10_cuda.so(+0x12f9f) [0x7f84cf9e4f9f] c10::cuda::CUDACachingAllocator::Native::local_raw_delete()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10_cuda.so(+0x1fac2) [0x7f84cf9f1ac2] c10::cuda::CUDACachingAllocator::Native::local_raw_delete()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_python.so(+0x5de5b0) [0x7f84cdfd65b0] c10::StorageImpl::~StorageImpl()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so(+0x6abdf) [0x7f84cf92dbdf] c10::intrusive_ptr<>::reset_()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so(_ZN3c1010TensorImplD1Ev+0x21b) [0x7f84cf926c3b] c10::TensorImpl::~TensorImpl()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libc10.so(_ZN3c1010TensorImplD0Ev+0x9) [0x7f84cf926de9] c10::TensorImpl::~TensorImpl()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_python.so(_ZNSt6vectorIN2at6TensorESaIS1_EED2Ev+0x88) [0x7f84cdfd8428] std::vector<>::~vector()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so(_ZN5torch8autograd6Engine11thread_mainERKSt10shared_ptrINS0_9GraphTaskEE+0x121) [0x7f84baabced1] torch::autograd::Engine::thread_main()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so(_ZN5torch8autograd6Engine11thread_initEiRKSt10shared_ptrINS0_10ReadyQueueEEb+0x13f) [0x7f84baab43cf] torch::autograd::Engine::thread_init()
[36m(WorkerDict pid=3300755)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/lib/libtorch_python.so(_ZN5torch8autograd6python12PythonEngine11thread_initEiRKSt10shared_ptrINS0_10ReadyQueueEEb+0x5c) [0x7f84ce268cac] torch::autograd::python::PythonEngine::thread_init()
[36m(WorkerDict pid=3300755)[0m /wang_ssd/zhihanliu/anaconda3/envs/verl/bin/../lib/libstdc++.so.6(+0xdbbf4) [0x7fa0aaa6fbf4] execute_native_thread_routine
[36m(WorkerDict pid=3300755)[0m /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7fa0acd5eac3]
[36m(WorkerDict pid=3300755)[0m /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7fa0acdf0850]
[36m(WorkerDict pid=3300755)[0m 
[36m(WorkerDict pid=3300755)[0m *** SIGABRT received at time=1740348872 on cpu 75 ***
[36m(WorkerDict pid=3300755)[0m PC: @     0x7fa0acd609fc  (unknown)  pthread_kill
[36m(WorkerDict pid=3300755)[0m     @     0x7fa0acd0c520  (unknown)  (unknown)
[36m(WorkerDict pid=3300755)[0m [2025-02-23 16:14:32,481 E 3300755 3304078] logging.cc:460: *** SIGABRT received at time=1740348872 on cpu 75 ***
[36m(WorkerDict pid=3300755)[0m [2025-02-23 16:14:32,481 E 3300755 3304078] logging.cc:460: PC: @     0x7fa0acd609fc  (unknown)  pthread_kill
[36m(WorkerDict pid=3300755)[0m [2025-02-23 16:14:32,481 E 3300755 3304078] logging.cc:460:     @     0x7fa0acd0c520  (unknown)  (unknown)
[36m(WorkerDict pid=3300755)[0m Fatal Python error: Aborted
[36m(WorkerDict pid=3300755)[0m 
[36m(WorkerDict pid=3300755)[0m Stack (most recent call first):
[36m(WorkerDict pid=3300755)[0m   <no Python frame>
[36m(WorkerDict pid=3300755)[0m 
[36m(WorkerDict pid=3300755)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sentencepiece._sentencepiece, PIL._imagingft, msgspec._core, regex._regex, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, pyarrow._json, zmq.backend.cython._zmq, cuda_utils, __triton_launcher (total: 91)

[36m(main_task pid=3300064)[0m 2x = 34
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m x = 17
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m 
[36m(main_task pid=3300064)[0m Next, we substitute \( x = 17 \) back into the first equation to find \( y \):
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m 17 + y = 25
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m y = 25 - 17
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m y = 8
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m 
[36m(main_task pid=3300064)[0m Now that we have the values of \( x \) and \( y \), we can find their product:
[36m(main_task pid=3300064)[0m \[
[36m(main_task pid=3300064)[0m x \cdot y = 17 \cdot 8 = 136
[36m(main_task pid=3300064)[0m \]
[36m(main_task pid=3300064)[0m 
[36m(main_task pid=3300064)[0m Thus, the product of the two numbers is \(\boxed{136}\).<|im_end|>
[36m(main_task pid=3300064)[0m ('Initial validation metrics: '
[36m(main_task pid=3300064)[0m  "{'val/test_score/DigitalLearningGmbH/MATH-lighteval': 0.6348539415766307}")
[36m(main_task pid=3300064)[0m step:0 - val/test_score/DigitalLearningGmbH/MATH-lighteval:0.635
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffc2a179ddb07a9f1f91b2587601000000 Worker ID: 4e51e044eaf09439408aeb1b06d283d4898a5400e63b3aae237f715a Node ID: c087c3c27e0a4314bb7da861fa32d89abfd7c4aec6680414b96edb46 Worker IP address: 10.14.38.55 Worker port: 32901 Worker PID: 3300755 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'algorithm.optimism_coef=0.01', 'algorithm.optimistic_actor=True', 'data.train_files=/wang_ssd/zhihanliu/data//train.parquet', 'data.val_files=/wang_ssd/zhihanliu/data//test.parquet', 'data.custom_temp_dir=/wang_ssd/zhihanliu/tmp/ray/', 'data.train_batch_size=1024', 'data.val_batch_size=1024', 'data.max_prompt_length=1024', 'data.max_response_length=2048', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=256', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=64', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=5', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=64', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=Qwen2.5-3B-Instruct-GRPO', 'trainer.experiment_name=MATH-mex0.01', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=10', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 240, in <module>
    main()
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 128, in main
    run_ppo(config)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 142, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ActorDiedError): [36mray::main_task()[39m (pid=3300064, ip=10.14.38.55)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 236, in main_task
    trainer.fit()
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/ppo/ray_trainer.py", line 979, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
	class_name: create_colocated_worker_cls.<locals>.WorkerDict
	actor_id: c2a179ddb07a9f1f91b2587601000000
	pid: 3300755
	name: jeGWmiWorkerDict_0:5
	namespace: 49b66440-8194-445e-a212-c0ca01d0cc6d
	ip: 10.14.38.55
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
Loading the DigitalLearningGmbH/MATH-lighteval dataset from huggingface...
Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 246.80ba/s]
Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 386.59ba/s]
2025-02-23 20:24:45,038	INFO worker.py:1841 -- Started a local Ray instance.
[36m(main_task pid=3363624)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=3363624)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=3363624)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3363624)[0m                                                  'grad_offload': False,
[36m(main_task pid=3363624)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=3363624)[0m                                                  'param_offload': False,
[36m(main_task pid=3363624)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3363624)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=3363624)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=3363624)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=3363624)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=3363624)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3363624)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=3363624)[0m                                            'total_training_steps': -1,
[36m(main_task pid=3363624)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=3363624)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=3363624)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=3363624)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=3363624)[0m                                  'ppo_micro_batch_size_per_gpu': 8,
[36m(main_task pid=3363624)[0m                                  'ppo_mini_batch_size': 256,
[36m(main_task pid=3363624)[0m                                  'shuffle': False,
[36m(main_task pid=3363624)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=3363624)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3363624)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=3363624)[0m                                  'use_kl_loss': True},
[36m(main_task pid=3363624)[0m                        'hybrid_engine': True,
[36m(main_task pid=3363624)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3363624)[0m                                  'external_lib': None,
[36m(main_task pid=3363624)[0m                                  'override_config': {},
[36m(main_task pid=3363624)[0m                                  'path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3363624)[0m                                  'use_remove_padding': True},
[36m(main_task pid=3363624)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=3363624)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3363624)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3363624)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=3363624)[0m                                'log_prob_micro_batch_size_per_gpu': 32,
[36m(main_task pid=3363624)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3363624)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=3363624)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=3363624)[0m                                    'do_sample': True,
[36m(main_task pid=3363624)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=3363624)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=3363624)[0m                                    'enforce_eager': True,
[36m(main_task pid=3363624)[0m                                    'free_cache_engine': True,
[36m(main_task pid=3363624)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=3363624)[0m                                    'ignore_eos': False,
[36m(main_task pid=3363624)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=3363624)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3363624)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=3363624)[0m                                    'log_prob_micro_batch_size_per_gpu': 32,
[36m(main_task pid=3363624)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3363624)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=3363624)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=3363624)[0m                                    'n': 5,
[36m(main_task pid=3363624)[0m                                    'name': 'vllm',
[36m(main_task pid=3363624)[0m                                    'prompt_length': 1024,
[36m(main_task pid=3363624)[0m                                    'response_length': 2048,
[36m(main_task pid=3363624)[0m                                    'temperature': 1.0,
[36m(main_task pid=3363624)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=3363624)[0m                                    'top_k': -1,
[36m(main_task pid=3363624)[0m                                    'top_p': 1}},
[36m(main_task pid=3363624)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=3363624)[0m                'gamma': 1.0,
[36m(main_task pid=3363624)[0m                'kl_ctrl': {'kl_coef': 0.001,
[36m(main_task pid=3363624)[0m                            'kl_coef_correction': 0.01,
[36m(main_task pid=3363624)[0m                            'type': 'fixed'},
[36m(main_task pid=3363624)[0m                'kl_penalty': 'kl',
[36m(main_task pid=3363624)[0m                'lam': 1.0,
[36m(main_task pid=3363624)[0m                'optimism_coef': 0.0,
[36m(main_task pid=3363624)[0m                'optimistic_actor': False,
[36m(main_task pid=3363624)[0m                'optimistic_critic': False},
[36m(main_task pid=3363624)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=3363624)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3363624)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=3363624)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=3363624)[0m             'grad_clip': 1.0,
[36m(main_task pid=3363624)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3363624)[0m                       'external_lib': None,
[36m(main_task pid=3363624)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3363624)[0m                                       'grad_offload': False,
[36m(main_task pid=3363624)[0m                                       'optimizer_offload': False,
[36m(main_task pid=3363624)[0m                                       'param_offload': False,
[36m(main_task pid=3363624)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3363624)[0m                       'override_config': {},
[36m(main_task pid=3363624)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=3363624)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3363624)[0m                       'use_remove_padding': False},
[36m(main_task pid=3363624)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=3363624)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3363624)[0m                       'min_lr_ratio': None,[36m(main_task pid=3363624)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=3363624)[0m No module named 'vllm._version'
[36m(main_task pid=3363624)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3364109)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3364109)[0m No module named 'vllm._version'
[36m(pid=3364109)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3364475)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3364475)[0m No module named 'vllm._version'
[36m(pid=3364475)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=3364475)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3364475)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3364475)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.86it/s]
[36m(WorkerDict pid=3364475)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]
[36m(WorkerDict pid=3364474)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(pid=3364478)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=3364478)[0m No module named 'vllm._version'[32m [repeated 6x across cluster][0m
[36m(pid=3364478)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3364478)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364479)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364479)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.14it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364479)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.40it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.36it/s][32m [repeated 7x across cluster][0m

[36m(main_task pid=3363624)[0m                       'total_training_steps': -1,
[36m(main_task pid=3363624)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=3363624)[0m             'ppo_epochs': 1,
[36m(main_task pid=3363624)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=3363624)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=3363624)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=3363624)[0m             'ppo_mini_batch_size': 256,
[36m(main_task pid=3363624)[0m             'shuffle': False,
[36m(main_task pid=3363624)[0m             'strategy': 'fsdp',
[36m(main_task pid=3363624)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3363624)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=3363624)[0m  'data': {'custom_temp_dir': '/wang_ssd/zhihanliu/tmp/ray/',
[36m(main_task pid=3363624)[0m           'max_prompt_length': 1024,
[36m(main_task pid=3363624)[0m           'max_response_length': 2048,
[36m(main_task pid=3363624)[0m           'prompt_key': 'prompt',
[36m(main_task pid=3363624)[0m           'return_raw_chat': False,
[36m(main_task pid=3363624)[0m           'return_raw_input_ids': False,
[36m(main_task pid=3363624)[0m           'save_ppo_rollouts_path': '',
[36m(main_task pid=3363624)[0m           'shuffle': True,
[36m(main_task pid=3363624)[0m           'tokenizer': None,
[36m(main_task pid=3363624)[0m           'train_batch_size': 1024,
[36m(main_task pid=3363624)[0m           'train_files': '/wang_ssd/zhihanliu/data//train.parquet',
[36m(main_task pid=3363624)[0m           'val_batch_size': 1024,
[36m(main_task pid=3363624)[0m           'val_files': '/wang_ssd/zhihanliu/data//test.parquet'},
[36m(main_task pid=3363624)[0m  'reward_model': {'enable': False,
[36m(main_task pid=3363624)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3363624)[0m                   'max_length': None,
[36m(main_task pid=3363624)[0m                   'micro_batch_size': None,
[36m(main_task pid=3363624)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=3363624)[0m                   'model': {'external_lib': None,
[36m(main_task pid=3363624)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3363624)[0m                                             'min_num_params': 0,
[36m(main_task pid=3363624)[0m                                             'param_offload': False},
[36m(main_task pid=3363624)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3363624)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=3363624)[0m                             'use_remove_padding': False},
[36m(main_task pid=3363624)[0m                   'reward_manager': 'naive',
[36m(main_task pid=3363624)[0m                   'strategy': 'fsdp',
[36m(main_task pid=3363624)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3363624)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=3363624)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=3363624)[0m              'default_hdfs_dir': None,
[36m(main_task pid=3363624)[0m              'default_local_dir': 'checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH_baseline',
[36m(main_task pid=3363624)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=3363624)[0m              'experiment_name': 'MATH_baseline',
[36m(main_task pid=3363624)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=3363624)[0m              'n_gpus_per_node': 8,
[36m(main_task pid=3363624)[0m              'nnodes': 1,
[36m(main_task pid=3363624)[0m              'project_name': 'Qwen2.5-3B-Instruct-GRPO',
[36m(main_task pid=3363624)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=3363624)[0m              'resume_from_path': False,
[36m(main_task pid=3363624)[0m              'resume_mode': 'auto',
[36m(main_task pid=3363624)[0m              'save_freq': 10,
[36m(main_task pid=3363624)[0m              'test_freq': 10,
[36m(main_task pid=3363624)[0m              'total_epochs': 2,
[36m(main_task pid=3363624)[0m              'total_training_steps': None,
[36m(main_task pid=3363624)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=3363624)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=3363624)[0m original dataset len: 7500
[36m(main_task pid=3363624)[0m filter dataset len: 7493
[36m(main_task pid=3363624)[0m original dataset len: 5000
[36m(main_task pid=3363624)[0m filter dataset len: 4998
[36m(main_task pid=3363624)[0m Size of train dataloader: 7
[36m(main_task pid=3363624)[0m Size of val dataloader: 1
[36m(main_task pid=3363624)[0m Total training steps: 14
[36m(WorkerDict pid=3364109)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3364109)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3364109)[0m   "architectures": [
[36m(WorkerDict pid=3364109)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3364109)[0m   ],
[36m(WorkerDict pid=3364109)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3364109)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3364109)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3364109)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3364109)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3364109)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3364109)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3364109)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3364109)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3364109)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3364109)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3364109)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3364109)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3364109)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3364109)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3364109)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3364109)[0m   "sliding_window": null,
[36m(WorkerDict pid=3364109)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3364109)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3364109)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3364109)[0m   "use_cache": true,
[36m(WorkerDict pid=3364109)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3364109)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3364109)[0m }
[36m(WorkerDict pid=3364109)[0m 
[36m(WorkerDict pid=3364109)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3364109)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3364109)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fa53cbafec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fa53cbafd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3364109)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3364109)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3364109)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3364109)[0m   "architectures": [
[36m(WorkerDict pid=3364109)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3364109)[0m   ],
[36m(WorkerDict pid=3364109)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3364109)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3364109)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3364109)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3364109)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3364109)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3364109)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3364109)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3364109)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3364109)[0m   "num_attention_heads": 16,[36m(WorkerDict pid=3364480)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364480)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3364480)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.97s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3364474)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]
[36m(WorkerDict pid=3364475)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
[36m(WorkerDict pid=3364475)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3364475)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3364480)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it][32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3364475)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3364475)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=3364475)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3364475)[0m   warnings.warn(
[36m(WorkerDict pid=3364477)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 14x across cluster][0m
[36m(WorkerDict pid=3364477)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364477)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 7x across cluster][0m
[36m(main_task pid=3363624)[0m wandb: Currently logged in as: zhliu0627 (northwestern_university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=3363624)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=3363624)[0m wandb: Tracking run with wandb version 0.19.5
[36m(main_task pid=3363624)[0m wandb: Run data is saved locally in /wang_ssd/zhihanliu/zhihan/verl/wandb/run-20250223_202547-9isryr3b
[36m(main_task pid=3363624)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=3363624)[0m wandb: Syncing run MATH_baseline
[36m(main_task pid=3363624)[0m wandb: ⭐️ View project at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO
[36m(main_task pid=3363624)[0m wandb: 🚀 View run at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO/runs/9isryr3b
[33m(raylet)[0m [2025-02-23 20:38:25,596 E 3359006 3359006] node_manager.cc:3180: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 604f5f8bdb71ec4b22393a38a52c64998a1b6dc9ec4ef79d8bde312e, IP: 10.14.38.55) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.14.38.55`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(WorkerDict pid=3364477)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364477)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364475)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
[36m(WorkerDict pid=3364475)[0m   warnings.warn('resource_tracker: There appear to be %d '

[36m(WorkerDict pid=3364109)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3364109)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3364109)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3364109)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3364109)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3364109)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3364109)[0m   "sliding_window": null,
[36m(WorkerDict pid=3364109)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3364109)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3364109)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3364109)[0m   "use_cache": true,
[36m(WorkerDict pid=3364109)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3364109)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3364109)[0m }
[36m(WorkerDict pid=3364109)[0m 
[36m(WorkerDict pid=3364479)[0m wrap_policy: functools.partial(<function _or_policy at 0x7eeb98613ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7eeb98613d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364109)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3364479)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364109)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fa53cbafec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fa53cbafd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3364475)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f8d9c627ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f8d9c627d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3364109)[0m Total steps: 14, num_warmup_steps: 0
[36m(WorkerDict pid=3364109)[0m Before building vllm rollout, memory allocated (GB): 1.4370465278625488, memory reserved (GB): 5.68359375
[36m(WorkerDict pid=3364109)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3364479)[0m wrap_policy: functools.partial(<function _or_policy at 0x7eeb98613ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7eeb98613d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3364475)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3364475)[0m INFO 02-23 20:25:36 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3364475)[0m INFO 02-23 20:25:36 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=3364475)[0m WARNING 02-23 20:25:36 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3364475)[0m local rank 0
[36m(WorkerDict pid=3364475)[0m INFO 02-23 20:25:36 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3364477)[0m INFO 02-23 20:25:37 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3364477)[0m INFO 02-23 20:25:37 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3364477)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3364479)[0m Total steps: 14, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364479)[0m Actor use_remove_padding=True[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3364475)[0m INFO 02-23 20:25:38 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f8d040e79d0>, local_subscribe_port=57239, remote_subscribe_port=None)
[36m(WorkerDict pid=3364109)[0m before init cache memory allocated: 4.700148736GB, reserved: 4.79199232GB
[36m(WorkerDict pid=3364109)[0m after init cache memory allocated: 30.89777152GB, reserved: 30.989615104GB
[36m(WorkerDict pid=3364477)[0m INFO 02-23 20:25:37 config.py:887] Defaulting to use ray for distributed inference[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364477)[0m INFO 02-23 20:25:37 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364477)[0m WARNING 02-23 20:25:37 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364477)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364479)[0m INFO 02-23 20:25:38 selector.py:115] Using XFormers backend.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3364479)[0m INFO 02-23 20:25:37 utils.py:1008] Found nccl from library libnccl.so.2[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364479)[0m INFO 02-23 20:25:37 pynccl.py:63] vLLM is using nccl==2.20.5[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3364479)[0m NCCL version 2.20.5+cuda12.4[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3364479)[0m INFO 02-23 20:25:38 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7eeae0e5a0d0>, local_subscribe_port=46249, remote_subscribe_port=None)[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3364475)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3364109)[0m After building vllm rollout, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(WorkerDict pid=3364109)[0m After building sharding manager, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(main_task pid=3363624)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(main_task pid=3363624)[0m Checkpoint tracker file does not exist: %s /wang_ssd/zhihanliu/zhihan/verl/checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH_baseline/latest_checkpointed_iteration.txt
[36m(main_task pid=3363624)[0m Training from scratch
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff2fd76e46f2ca345e8e3a1a7201000000 Worker ID: 786a07840e1b6044bbc693d6a5eb3e193166d15557f711de42ee024b Node ID: 604f5f8bdb71ec4b22393a38a52c64998a1b6dc9ec4ef79d8bde312e Worker IP address: 10.14.38.55 Worker port: 36277 Worker PID: 3364476 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[36m(WorkerDict pid=3364477)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/wang_ssd/zhihanliu/data//train.parquet', 'data.val_files=/wang_ssd/zhihanliu/data//test.parquet', 'data.custom_temp_dir=/wang_ssd/zhihanliu/tmp/ray/', 'data.train_batch_size=1024', 'data.val_batch_size=1024', 'data.max_prompt_length=1024', 'data.max_response_length=2048', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=256', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=5', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=Qwen2.5-3B-Instruct-GRPO', 'trainer.experiment_name=MATH_baseline', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=10', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 240, in <module>
    main()
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 128, in main
    run_ppo(config)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 142, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 921, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.14.38.55, ID: 604f5f8bdb71ec4b22393a38a52c64998a1b6dc9ec4ef79d8bde312e) where the task (task ID: fa72cddac1668f82f088b45242b36b7aed9e3f9701000000, name=main_task, pid=3363624, memory used=1.02GB) was running was 364.69GB / 376.55GB (0.968498), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c3cce33379d4e3c34f18e9bdc1ef3d84ef2ff5f74a17725f40f7ae4b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.14.38.55`. To see the logs of the worker, use `ray logs worker-c3cce33379d4e3c34f18e9bdc1ef3d84ef2ff5f74a17725f40f7ae4b*out -ip 10.14.38.55. Top 10 memory users:
PID	MEM(GB)	COMMAND
3364476	10.31	
3364474	10.31	ray::WorkerDict
3364479	10.30	ray::WorkerDict
3364109	10.29	ray::WorkerDict
3364475	10.29	ray::WorkerDict
3364480	10.28	ray::WorkerDict
3364478	10.12	ray::WorkerDict.actor_rollout_generate_sequences
3364477	10.11	ray::WorkerDict.actor_rollout_generate_sequences
3024259	1.04	/wang_ssd/zhihanliu/.vscode-server/cli/servers/Stable-cd4ee3b1c348a13bafd8f9ad8060705f6d4b9cba/serve...
3363624	1.02	ray::main_task
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff401747f8134f35c666b246dd01000000 Worker ID: 00ec408664ae19cf36eb3ca1a80febefd1a6e062acd1720bde6ac70c Node ID: 604f5f8bdb71ec4b22393a38a52c64998a1b6dc9ec4ef79d8bde312e Worker IP address: 10.14.38.55 Worker port: 34569 Worker PID: 3364475 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
Loading the DigitalLearningGmbH/MATH-lighteval dataset from huggingface...
Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 408.40ba/s]
Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 544.06ba/s]
2025-02-23 20:40:00,150	INFO worker.py:1841 -- Started a local Ray instance.
[36m(main_task pid=3379396)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=3379396)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=3379396)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3379396)[0m                                                  'grad_offload': False,
[36m(main_task pid=3379396)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=3379396)[0m                                                  'param_offload': False,
[36m(main_task pid=3379396)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3379396)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=3379396)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=3379396)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=3379396)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=3379396)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3379396)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=3379396)[0m                                            'total_training_steps': -1,
[36m(main_task pid=3379396)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=3379396)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=3379396)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=3379396)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=3379396)[0m                                  'ppo_micro_batch_size_per_gpu': 8,
[36m(main_task pid=3379396)[0m                                  'ppo_mini_batch_size': 256,
[36m(main_task pid=3379396)[0m                                  'shuffle': False,
[36m(main_task pid=3379396)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=3379396)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3379396)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=3379396)[0m                                  'use_kl_loss': True},
[36m(main_task pid=3379396)[0m                        'hybrid_engine': True,
[36m(main_task pid=3379396)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3379396)[0m                                  'external_lib': None,
[36m(main_task pid=3379396)[0m                                  'override_config': {},
[36m(main_task pid=3379396)[0m                                  'path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3379396)[0m                                  'use_remove_padding': True},
[36m(main_task pid=3379396)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=3379396)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3379396)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3379396)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=3379396)[0m                                'log_prob_micro_batch_size_per_gpu': 32,
[36m(main_task pid=3379396)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3379396)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=3379396)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=3379396)[0m                                    'do_sample': True,
[36m(main_task pid=3379396)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=3379396)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=3379396)[0m                                    'enforce_eager': True,
[36m(main_task pid=3379396)[0m                                    'free_cache_engine': True,
[36m(main_task pid=3379396)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=3379396)[0m                                    'ignore_eos': False,
[36m(main_task pid=3379396)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=3379396)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3379396)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=3379396)[0m                                    'log_prob_micro_batch_size_per_gpu': 32,
[36m(main_task pid=3379396)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3379396)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=3379396)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=3379396)[0m                                    'n': 5,
[36m(main_task pid=3379396)[0m                                    'name': 'vllm',
[36m(main_task pid=3379396)[0m                                    'prompt_length': 1024,
[36m(main_task pid=3379396)[0m                                    'response_length': 2048,
[36m(main_task pid=3379396)[0m                                    'temperature': 1.0,
[36m(main_task pid=3379396)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=3379396)[0m                                    'top_k': -1,
[36m(main_task pid=3379396)[0m                                    'top_p': 1}},
[36m(main_task pid=3379396)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=3379396)[0m                'gamma': 1.0,
[36m(main_task pid=3379396)[0m                'kl_ctrl': {'kl_coef': 0.001,
[36m(main_task pid=3379396)[0m                            'kl_coef_correction': 0.01,
[36m(main_task pid=3379396)[0m                            'type': 'fixed'},
[36m(main_task pid=3379396)[0m                'kl_penalty': 'kl',
[36m(main_task pid=3379396)[0m                'lam': 1.0,
[36m(main_task pid=3379396)[0m                'optimism_coef': 0.01,
[36m(main_task pid=3379396)[0m                'optimistic_actor': True,
[36m(main_task pid=3379396)[0m                'optimistic_critic': False},
[36m(main_task pid=3379396)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=3379396)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3379396)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=3379396)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=3379396)[0m             'grad_clip': 1.0,
[36m(main_task pid=3379396)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3379396)[0m                       'external_lib': None,
[36m(main_task pid=3379396)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3379396)[0m                                       'grad_offload': False,
[36m(main_task pid=3379396)[0m                                       'optimizer_offload': False,
[36m(main_task pid=3379396)[0m                                       'param_offload': False,
[36m(main_task pid=3379396)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3379396)[0m                       'override_config': {},
[36m(main_task pid=3379396)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=3379396)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3379396)[0m                       'use_remove_padding': False},
[36m(main_task pid=3379396)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=3379396)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3379396)[0m                       'min_lr_ratio': None,[36m(main_task pid=3379396)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=3379396)[0m No module named 'vllm._version'
[36m(main_task pid=3379396)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3380095)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3380095)[0m No module named 'vllm._version'
[36m(pid=3380095)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=3380367)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=3380367)[0m No module named 'vllm._version'
[36m(pid=3380367)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=3380372)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3380372)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3380095)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.77it/s]
[36m(WorkerDict pid=3380095)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.98it/s]
[36m(WorkerDict pid=3380368)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(pid=3380369)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 6x across cluster][0m
[36m(pid=3380369)[0m No module named 'vllm._version'[32m [repeated 6x across cluster][0m
[36m(pid=3380369)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3380370)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380370)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380368)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380370)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s][32m [repeated 7x across cluster][0m

[36m(main_task pid=3379396)[0m                       'total_training_steps': -1,
[36m(main_task pid=3379396)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=3379396)[0m             'ppo_epochs': 1,
[36m(main_task pid=3379396)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=3379396)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=3379396)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=3379396)[0m             'ppo_mini_batch_size': 256,
[36m(main_task pid=3379396)[0m             'shuffle': False,
[36m(main_task pid=3379396)[0m             'strategy': 'fsdp',
[36m(main_task pid=3379396)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3379396)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=3379396)[0m  'data': {'custom_temp_dir': '/wang_ssd/zhihanliu/tmp/ray/',
[36m(main_task pid=3379396)[0m           'max_prompt_length': 1024,
[36m(main_task pid=3379396)[0m           'max_response_length': 2048,
[36m(main_task pid=3379396)[0m           'prompt_key': 'prompt',
[36m(main_task pid=3379396)[0m           'return_raw_chat': False,
[36m(main_task pid=3379396)[0m           'return_raw_input_ids': False,
[36m(main_task pid=3379396)[0m           'save_ppo_rollouts_path': '',
[36m(main_task pid=3379396)[0m           'shuffle': True,
[36m(main_task pid=3379396)[0m           'tokenizer': None,
[36m(main_task pid=3379396)[0m           'train_batch_size': 1024,
[36m(main_task pid=3379396)[0m           'train_files': '/wang_ssd/zhihanliu/data//train.parquet',
[36m(main_task pid=3379396)[0m           'val_batch_size': 1024,
[36m(main_task pid=3379396)[0m           'val_files': '/wang_ssd/zhihanliu/data//test.parquet'},
[36m(main_task pid=3379396)[0m  'reward_model': {'enable': False,
[36m(main_task pid=3379396)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3379396)[0m                   'max_length': None,
[36m(main_task pid=3379396)[0m                   'micro_batch_size': None,
[36m(main_task pid=3379396)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=3379396)[0m                   'model': {'external_lib': None,
[36m(main_task pid=3379396)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3379396)[0m                                             'min_num_params': 0,
[36m(main_task pid=3379396)[0m                                             'param_offload': False},
[36m(main_task pid=3379396)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct',
[36m(main_task pid=3379396)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=3379396)[0m                             'use_remove_padding': False},
[36m(main_task pid=3379396)[0m                   'reward_manager': 'naive',
[36m(main_task pid=3379396)[0m                   'strategy': 'fsdp',
[36m(main_task pid=3379396)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3379396)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=3379396)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=3379396)[0m              'default_hdfs_dir': None,
[36m(main_task pid=3379396)[0m              'default_local_dir': 'checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH-mex0.01',
[36m(main_task pid=3379396)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=3379396)[0m              'experiment_name': 'MATH-mex0.01',
[36m(main_task pid=3379396)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=3379396)[0m              'n_gpus_per_node': 8,
[36m(main_task pid=3379396)[0m              'nnodes': 1,
[36m(main_task pid=3379396)[0m              'project_name': 'Qwen2.5-3B-Instruct-GRPO',
[36m(main_task pid=3379396)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=3379396)[0m              'resume_from_path': False,
[36m(main_task pid=3379396)[0m              'resume_mode': 'auto',
[36m(main_task pid=3379396)[0m              'save_freq': 10,
[36m(main_task pid=3379396)[0m              'test_freq': 10,
[36m(main_task pid=3379396)[0m              'total_epochs': 2,
[36m(main_task pid=3379396)[0m              'total_training_steps': None,
[36m(main_task pid=3379396)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=3379396)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=3379396)[0m original dataset len: 7500
[36m(main_task pid=3379396)[0m filter dataset len: 7493
[36m(main_task pid=3379396)[0m original dataset len: 5000
[36m(main_task pid=3379396)[0m filter dataset len: 4998
[36m(main_task pid=3379396)[0m Size of train dataloader: 7
[36m(main_task pid=3379396)[0m Size of val dataloader: 1
[36m(main_task pid=3379396)[0m Total training steps: 14
[36m(WorkerDict pid=3380095)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3380095)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3380095)[0m   "architectures": [
[36m(WorkerDict pid=3380095)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3380095)[0m   ],
[36m(WorkerDict pid=3380095)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3380095)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3380095)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3380095)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3380095)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3380095)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3380095)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3380095)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3380095)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3380095)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3380095)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3380095)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3380095)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3380095)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3380095)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3380095)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3380095)[0m   "sliding_window": null,
[36m(WorkerDict pid=3380095)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3380095)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3380095)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3380095)[0m   "use_cache": true,
[36m(WorkerDict pid=3380095)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3380095)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3380095)[0m }
[36m(WorkerDict pid=3380095)[0m 
[36m(WorkerDict pid=3380095)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3380095)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3380095)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fcfd57e7ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fcfd57e7d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3380095)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3380370)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fca1155fec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fca1155fd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3380095)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3380095)[0m   "_name_or_path": "Qwen/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=3380095)[0m   "architectures": [
[36m(WorkerDict pid=3380095)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3380095)[0m   ],
[36m(WorkerDict pid=3380095)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3380095)[0m   "eos_token_id": 151645,[36m(WorkerDict pid=3380095)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380095)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3380366)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.26s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3380368)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]
[36m(WorkerDict pid=3380370)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]
[36m(WorkerDict pid=3380372)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3380372)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3380366)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it][32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3380372)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=3380368)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3380368)[0m   warnings.warn(
[36m(WorkerDict pid=3380366)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3380366)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380366)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 7x across cluster][0m
[36m(main_task pid=3379396)[0m wandb: Currently logged in as: zhliu0627 (northwestern_university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=3379396)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=3379396)[0m wandb: Tracking run with wandb version 0.19.5
[36m(main_task pid=3379396)[0m wandb: Run data is saved locally in /wang_ssd/zhihanliu/zhihan/verl/wandb/run-20250223_204155-zqnh7uyn
[36m(main_task pid=3379396)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=3379396)[0m wandb: Syncing run MATH-mex0.01
[36m(main_task pid=3379396)[0m wandb: ⭐️ View project at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO
[36m(main_task pid=3379396)[0m wandb: 🚀 View run at https://wandb.ai/northwestern_university/Qwen2.5-3B-Instruct-GRPO/runs/zqnh7uyn

[36m(WorkerDict pid=3380095)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3380095)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3380095)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3380095)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3380095)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3380095)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3380095)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3380095)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3380095)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3380095)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3380095)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3380095)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3380095)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3380095)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3380095)[0m   "sliding_window": null,
[36m(WorkerDict pid=3380095)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3380095)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3380095)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3380095)[0m   "use_cache": true,
[36m(WorkerDict pid=3380095)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3380095)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3380095)[0m }
[36m(WorkerDict pid=3380095)[0m 
[36m(WorkerDict pid=3380095)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=3380370)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380095)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fcfd57e7ec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fcfd57e7d80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3380372)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f9fe5eebec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f9fe5eebd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3380371)[0m Total steps: 14, num_warmup_steps: 0
[36m(WorkerDict pid=3380371)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3380370)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fca1155fec0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fca1155fd80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3380368)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3380095)[0m Before building vllm rollout, memory allocated (GB): 1.4370465278625488, memory reserved (GB): 5.68359375
[36m(WorkerDict pid=3380370)[0m INFO 02-23 20:41:36 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3380370)[0m INFO 02-23 20:41:36 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=3380370)[0m WARNING 02-23 20:41:36 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3380370)[0m Total steps: 14, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380370)[0m Actor use_remove_padding=True[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3380372)[0m local rank 0
[36m(WorkerDict pid=3380372)[0m INFO 02-23 20:41:37 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=3380095)[0m INFO 02-23 20:41:39 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3380095)[0m INFO 02-23 20:41:39 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3380371)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3380367)[0m INFO 02-23 20:41:39 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc91a412790>, local_subscribe_port=42155, remote_subscribe_port=None)
[36m(WorkerDict pid=3380095)[0m before init cache memory allocated: 4.700148736GB, reserved: 4.79199232GB
[36m(WorkerDict pid=3380366)[0m INFO 02-23 20:41:37 config.py:887] Defaulting to use ray for distributed inference[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380366)[0m INFO 02-23 20:41:37 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380366)[0m WARNING 02-23 20:41:37 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380366)[0m local rank 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380370)[0m INFO 02-23 20:41:40 selector.py:115] Using XFormers backend.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3380370)[0m INFO 02-23 20:41:39 utils.py:1008] Found nccl from library libnccl.so.2[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380370)[0m INFO 02-23 20:41:39 pynccl.py:63] vLLM is using nccl==2.20.5[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380367)[0m NCCL version 2.20.5+cuda12.4[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3380369)[0m INFO 02-23 20:41:40 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f03844c6e90>, local_subscribe_port=37391, remote_subscribe_port=None)[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3380368)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3380095)[0m after init cache memory allocated: 30.89777152GB, reserved: 30.989615104GB
[36m(WorkerDict pid=3380095)[0m After building vllm rollout, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(WorkerDict pid=3380095)[0m After building sharding manager, memory allocated (GB): 25.851234436035156, memory reserved (GB): 28.861328125
[36m(main_task pid=3379396)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(main_task pid=3379396)[0m Checkpoint tracker file does not exist: %s /wang_ssd/zhihanliu/zhihan/verl/checkpoints/Qwen2.5-3B-Instruct-GRPO/MATH-mex0.01/latest_checkpointed_iteration.txt
[36m(main_task pid=3379396)[0m Training from scratch
[36m(main_task pid=3379396)[0m validation generation end
[36m(WorkerDict pid=3380370)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
[36m(main_task pid=3379396)[0m <|im_start|>system
[36m(main_task pid=3379396)[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
[36m(main_task pid=3379396)[0m <|im_start|>user
[36m(main_task pid=3379396)[0m Compute $a+b+c,$ given that $a,$ $b,$ and $c$ are the roots of \[\frac{1}{x} + 5x^2 = 6x - 24.\] Let's think step by step and output the final answer within \boxed{}.<|im_end|>
[36m(main_task pid=3379396)[0m <|im_start|>assistant
[36m(main_task pid=3379396)[0m To solve for \(a + b + c\) where \(a\), \(b\), and \(c\) are the roots of the equation \(\frac{1}{x} + 5x^2 = 6x - 24\), we first need to rewrite the equation in a standard polynomial form. Let's start by eliminating the fraction by multiplying through by \(x\):
[36m(main_task pid=3379396)[0m 
[36m(main_task pid=3379396)[0m \[1 + 5x^3 = 6x^2 - 24x.\]
[36m(main_task pid=3379396)[0m 
[36m(main_task pid=3379396)[0m Rearranging all terms to one side of the equation, we get:
[36m(main_task pid=3379396)[0m 
[36m(main_task pid=3379396)[0m \[5x^3 - 6x^2 + 24x + 1 = 0.\]
[36m(main_task pid=3379396)[0m 
[36m(main_task pid=3379396)[0m This is a cubic equation in the standard form \(5x^3 - 6x^2 + 24x + 1 = 0\). According to Vieta's formulas, for a cubic equation of the form \(ax^3 + bx^2 + cx + d = 0\), the sum of the roots \(a + b + c\) is given by \(-\frac{b}{a}\). Here, \(a = 5\), \(b = -6\), \(c = 24\), and \(d = 1\). Therefore, the sum of the roots is:[36m(WorkerDict pid=3380095)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=3380095)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=3380370)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380370)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380095)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.[32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3380095)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined][32m [repeated 8x across cluster][0m

[36m(main_task pid=3379396)[0m 
[36m(main_task pid=3379396)[0m \[a + b + c = -\frac{-6}{5} = \frac{6}{5}.\]
[36m(main_task pid=3379396)[0m 
[36m(main_task pid=3379396)[0m Thus, the final answer is:
[36m(main_task pid=3379396)[0m 
[36m(main_task pid=3379396)[0m \[\boxed{\frac{6}{5}}.\]<|im_end|>
[36m(main_task pid=3379396)[0m ('Initial validation metrics: '
[36m(main_task pid=3379396)[0m  "{'val/test_score/DigitalLearningGmbH/MATH-lighteval': 0.6300520208083233}")
[36m(main_task pid=3379396)[0m step:0 - val/test_score/DigitalLearningGmbH/MATH-lighteval:0.630
[36m(main_task pid=3379396)[0m step:1 - global_seqlen/min:414772.000 - global_seqlen/max:464647.000 - global_seqlen/minmax_diff:49875.000 - global_seqlen/balanced_min:439538.000 - global_seqlen/balanced_max:439539.000 - global_seqlen/mean:439538.375 - original_advantages:-0.004 - optimistic_coef:0.010 - optimistic_advantages:inf - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.000 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.640 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.640 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.015 - critic/advantages/max:1.789 - critic/advantages/min:-1.789 - critic/returns/mean:-0.015 - critic/returns/max:1.789 - critic/returns/min:-1.789 - response_length/mean:568.192 - response_length/max:2048.000 - response_length/min:101.000 - response_length/clip_ratio:0.005 - prompt_length/mean:118.587 - prompt_length/max:1004.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:643.471 - timing_s/old_log_prob:185.958 - timing_s/ref:186.122 - timing_s/adv:2.227 - timing_s/update_actor:761.209 - timing_s/step:1779.813 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/gen:0.221 - timing_per_token_ms/adv:0.001 - timing_per_token_ms/update_actor:0.216
[36m(main_task pid=3379396)[0m step:2 - global_seqlen/min:1380170.000 - global_seqlen/max:1392810.000 - global_seqlen/minmax_diff:12640.000 - global_seqlen/balanced_min:1388251.000 - global_seqlen/balanced_max:1388252.000 - global_seqlen/mean:1388251.875 - original_advantages:0.000 - optimistic_coef:0.010 - optimistic_advantages:0.000 - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.000 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.000 - critic/score/max:0.000 - critic/score/min:0.000 - critic/rewards/mean:0.000 - critic/rewards/max:0.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.000 - critic/advantages/max:0.000 - critic/advantages/min:0.000 - critic/returns/mean:0.000 - critic/returns/max:0.000 - critic/returns/min:0.000 - response_length/mean:2048.000 - response_length/max:2048.000 - response_length/min:2048.000 - response_length/clip_ratio:1.000 - prompt_length/mean:121.144 - prompt_length/max:906.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:2832.492 - timing_s/old_log_prob:594.298 - timing_s/ref:592.099 - timing_s/adv:3.384 - timing_s/update_actor:2293.977 - timing_s/step:6316.756 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/gen:0.270 - timing_per_token_ms/adv:0.000 - timing_per_token_ms/update_actor:0.207
[36m(main_task pid=3379396)[0m step:3 - global_seqlen/min:1373715.000 - global_seqlen/max:1393680.000 - global_seqlen/minmax_diff:19965.000 - global_seqlen/balanced_min:1384261.000 - global_seqlen/balanced_max:1384262.000 - global_seqlen/mean:1384261.250 - original_advantages:0.000 - optimistic_coef:0.010 - optimistic_advantages:0.000 - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.000 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.000 - critic/score/max:0.000 - critic/score/min:0.000 - critic/rewards/mean:0.000 - critic/rewards/max:0.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.000 - critic/advantages/max:0.000 - critic/advantages/min:0.000 - critic/returns/mean:0.000 - critic/returns/max:0.000 - critic/returns/min:0.000 - response_length/mean:2048.000 - response_length/max:2048.000 - response_length/min:2048.000 - response_length/clip_ratio:1.000 - prompt_length/mean:114.908 - prompt_length/max:858.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:2833.162 - timing_s/old_log_prob:592.877 - timing_s/ref:588.428 - timing_s/adv:3.134 - timing_s/update_actor:2251.352 - timing_s/step:6269.471 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/gen:0.270 - timing_per_token_ms/adv:0.000 - timing_per_token_ms/update_actor:0.203
[36m(main_task pid=3379396)[0m step:4 - global_seqlen/min:1378220.000 - global_seqlen/max:1389990.000 - global_seqlen/minmax_diff:11770.000 - global_seqlen/balanced_min:1386470.000 - global_seqlen/balanced_max:1386470.000 - global_seqlen/mean:1386470.000 - original_advantages:0.000 - optimistic_coef:0.010 - optimistic_advantages:0.000 - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.000 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.000 - critic/score/max:0.000 - critic/score/min:0.000 - critic/rewards/mean:0.000 - critic/rewards/max:0.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.000 - critic/advantages/max:0.000 - critic/advantages/min:0.000 - critic/returns/mean:0.000 - critic/returns/max:0.000 - critic/returns/min:0.000 - response_length/mean:2048.000 - response_length/max:2048.000 - response_length/min:2048.000 - response_length/clip_ratio:1.000 - prompt_length/mean:118.359 - prompt_length/max:884.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:2831.184 - timing_s/old_log_prob:593.544 - timing_s/ref:593.468 - timing_s/adv:3.139 - timing_s/update_actor:2290.394 - timing_s/step:6312.285 - timing_per_token_ms/ref:0.054 - timing_per_token_ms/gen:0.270 - timing_per_token_ms/adv:0.000 - timing_per_token_ms/update_actor:0.206
[36m(main_task pid=3379396)[0m step:5 - global_seqlen/min:1379150.000 - global_seqlen/max:1395715.000 - global_seqlen/minmax_diff:16565.000 - global_seqlen/balanced_min:1385773.000 - global_seqlen/balanced_max:1385774.000 - global_seqlen/mean:1385773.750 - original_advantages:0.000 - optimistic_coef:0.010 - optimistic_advantages:0.000 - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.000 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.001 - critic/score/max:1.000 - critic/score/min:0.000 - critic/rewards/mean:0.001 - critic/rewards/max:1.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.000 - critic/advantages/max:0.000 - critic/advantages/min:0.000 - critic/returns/mean:0.000 - critic/returns/max:0.000 - critic/returns/min:0.000 - response_length/mean:2048.000 - response_length/max:2048.000 - response_length/min:2048.000 - response_length/clip_ratio:1.000 - prompt_length/mean:117.271 - prompt_length/max:882.000 - prompt_length/min:51.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:2829.519 - timing_s/old_log_prob:592.845 - timing_s/ref:587.539 - timing_s/adv:3.413 - timing_s/update_actor:2252.295 - timing_s/step:6266.130 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/gen:0.270 - timing_per_token_ms/adv:0.000 - timing_per_token_ms/update_actor:0.203
[36m(main_task pid=3379396)[0m step:6 - global_seqlen/min:1380110.000 - global_seqlen/max:1392600.000 - global_seqlen/minmax_diff:12490.000 - global_seqlen/balanced_min:1386882.000 - global_seqlen/balanced_max:1386883.000 - global_seqlen/mean:1386882.500 - original_advantages:0.000 - optimistic_coef:0.010 - optimistic_advantages:0.000 - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.000 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.000 - critic/score/max:0.000 - critic/score/min:0.000 - critic/rewards/mean:0.000 - critic/rewards/max:0.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.000 - critic/advantages/max:0.000 - critic/advantages/min:0.000 - critic/returns/mean:0.000 - critic/returns/max:0.000 - critic/returns/min:0.000 - response_length/mean:2048.000 - response_length/max:2048.000 - response_length/min:2048.000 - response_length/clip_ratio:1.000 - prompt_length/mean:119.004 - prompt_length/max:947.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:2837.527 - timing_s/old_log_prob:593.305 - timing_s/ref:589.576 - timing_s/adv:3.054 - timing_s/update_actor:2292.189 - timing_s/step:6316.129 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/gen:0.271 - timing_per_token_ms/adv:0.000 - timing_per_token_ms/update_actor:0.207
[36m(main_task pid=3379396)[0m step:7 - global_seqlen/min:1380680.000 - global_seqlen/max:1393260.000 - global_seqlen/minmax_diff:12580.000 - global_seqlen/balanced_min:1387803.000 - global_seqlen/balanced_max:1387804.000 - global_seqlen/mean:1387803.750 - original_advantages:0.000 - optimistic_coef:0.010 - optimistic_advantages:0.000 - actor/kl_loss:nan - actor/kl_coef:0.001 - actor/entropy_loss:nan - actor/pg_loss:nan - actor/pg_clipfrac:0.000 - actor/ppo_kl:nan - actor/grad_norm:nan - mfu/actor:0.000 - actor/lr:0.000 - critic/score/mean:0.000 - critic/score/max:0.000 - critic/score/min:0.000 - critic/rewards/mean:0.000 - critic/rewards/max:0.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.000 - critic/advantages/max:0.000 - critic/advantages/min:0.000 - critic/returns/mean:0.000 - critic/returns/max:0.000 - critic/returns/min:0.000 - response_length/mean:2048.000 - response_length/max:2048.000 - response_length/min:2048.000 - response_length/clip_ratio:1.000 - prompt_length/mean:120.443 - prompt_length/max:935.000 - prompt_length/min:52.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:2830.544 - timing_s/old_log_prob:593.701 - timing_s/ref:592.841 - timing_s/adv:3.107 - timing_s/update_actor:2289.780 - timing_s/step:6310.427 - timing_per_token_ms/ref:0.053 - timing_per_token_ms/gen:0.270 - timing_per_token_ms/adv:0.000 - timing_per_token_ms/update_actor:0.206
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffffc66b3b5ed7f714ea9efd9e501000000 Worker ID: 9f080519c670f8c9e0a4b77786b059b927ce771a982c110f27b6f991 Node ID: b31f25014d5ebc82f2732e0f994cf43fa2ae1a17b1f75215c2e740ef Worker IP address: 10.14.38.55 Worker port: 37589 Worker PID: 3380095 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1. The process receives a SIGTERM.
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'algorithm.optimism_coef=0.01', 'algorithm.optimistic_actor=True', 'data.train_files=/wang_ssd/zhihanliu/data//train.parquet', 'data.val_files=/wang_ssd/zhihanliu/data//test.parquet', 'data.custom_temp_dir=/wang_ssd/zhihanliu/tmp/ray/', 'data.train_batch_size=1024', 'data.val_batch_size=1024', 'data.max_prompt_length=1024', 'data.max_response_length=2048', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=256', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=5', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=Qwen2.5-3B-Instruct-GRPO', 'trainer.experiment_name=MATH-mex0.01', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=10', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 240, in <module>
    main()
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 128, in main
    run_ppo(config)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 142, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ActorDiedError): [36mray::main_task()[39m (pid=3379396, ip=10.14.38.55)
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/main_ppo.py", line 236, in main_task
    trainer.fit()
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/trainer/ppo/ray_trainer.py", line 979, in fit
  File "/wang_ssd/zhihanliu/zhihan/verl/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
	class_name: create_colocated_worker_cls.<locals>.WorkerDict
	actor_id: fc66b3b5ed7f714ea9efd9e501000000
	pid: 3380095
	name: DcMD2xWorkerDict_0:0
	namespace: 294021d7-3837-4e85-bfe7-5feacda92a7a
	ip: 10.14.38.55
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1. The process receives a SIGTERM.
[36m(WorkerDict pid=3380370)[0m /home/zhihanliu/anaconda3/envs/verl/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3380370)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined][32m [repeated 7x across cluster][0m
